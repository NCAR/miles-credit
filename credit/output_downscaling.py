"""
output_downscaling.py
-------------------------------------------------------
Content:
    - OutputWrangler
    - OutputWriter
"""

import os
import yaml
import logging
import traceback
import numpy as np
import xarray as xr

from dataclasses import dataclass, field
from typing import Dict, List

from credit.datamap import VarDict
from credit.data_downscaling import DownscalingDataset

logger = logging.getLogger(__name__)


@dataclass
class OutputWrangler:
    dataset:        DownscalingDataset
    templates:      Dict  # {'dir': path, 'files': {dataset1: file1, dataset2: file2}}
    output_dir:     str
    # save_vars: List[str] = None  # todo: allow yaml to subset output vars

    def __post_init__(self):
        # templates has been validated by parser; we can assume its structure is good

        self.writers = {}
        for dset in self.templates['files']:
            print(dset)
            dmap = self.dataset.datasets[dset]['datamap']
            print(dmap.variables)

            tpath = os.path.join(self.templates['dir'], self.templates['files'][dset])
            
            self.writers[dset] = OutputWriter(
                template_path = tpath,
                variables = dmap.variables,
                dim = dmap.dim,
                zstride = dmap.zstride
            )

    def process(self, y_pred, dates):
        # y_pred is tensor generated by model(x)
        # dates is a Sample['dates'] dict

        hlen = self.dataset.history_len
        
        datadict = self.dataset.revert(y_pred)

        # go with first datetime if forecast_len is > 1
        datestamp = dates['cf_datetimes'][hlen].replace(" ", "_")
        
        outdate = {k: dates[k] for k in ['calendar','units']}
        outdate['time'] = dates['time'][hlen:]
                
        for dset in datadict:
            outfile = f"{dset}.{datestamp}.nc"
            outpath = os.path.join(self.output_dir, outfile)
            try:
                self.writers[dset].write(
                    data = datadict[dset],
                    newtime = outdate,
                    output_path = outpath
                )
                
                logger.info(f"Saved prediction to {outfile}")
            except Exception as e:
                print(traceback.format_exc())
                raise e


# OutputWriter class

@dataclass
class OutputWriter:
    template_path: str
    dim:       str = "2D"
    variables: VarDict[str, List] = field(default_factory=list)
    zstride:   int = 1
    noop:      bool = False

    def __post_init__(self):
        # generate list of output variables
        self.outvars = []
        for usage in ("prognostic", "diagnostic"):
            if usage in self.variables:
                self.outvars = self.outvars + self.variables[usage]

        if len(self.outvars) == 0:
            self.noop = True
        else:
            self.noop = False
                        
            # create template xarray object
            self.template = xr.open_dataset(self.template_path, decode_times=False)
            self.template.load()
            self.template.close()
            
            for v in self.template.data_vars:
                if v in self.outvars:
                    if self.dim == "3D" and self.zstride != 0:
                        #todo: subset z-levels
                        pass
                    # this should never matter, but just in case
                    self.template[v][:] = np.nan                    
                else:
                    self.template = self.template.drop(v)

    def write(self, data, newtime, output_path):

        if self.noop:
            pass
        else:
            result = self.template.copy(deep=True, data=data)

            result.coords['time'] = newtime['time']
            result.coords['time'].attrs['units'] = newtime['units']
            result.coords['time'].attrs['calendar'] = newtime['calendar']

            result.to_netcdf(output_path)


#def save_netcdf_increment(
#    darray_upper_air: xr.DataArray,
#    darray_single_level: xr.DataArray,
#    nc_filename: str,
#    forecast_hour: int,
#    meta_data: dict,
#    conf: dict,
#):
#    """
#    Save CREDIT model prediction output to netCDF file. Also performs pressure level
#    interpolation on the output if you wish.
#
#    Args:
#        darray_upper_air (xr.DataArray): upper air variable predictions
#        darray_single_level (xr.DataArray): surface variable predictions
#        nc_filename (str): file description to go into output filenames
#        forecast_hour (int):  how many hours since the initialization of the model.
#        meta_data (dict): metadata dictionary for output variables
#        conf (dict): configuration dictionary for training and/or rollout
#
#    """
#    try:
#        """
#        Save increment to a unique NetCDF file using Dask for parallel processing.
#        """
#        # Convert DataArrays to Datasets
#        ds_upper = darray_upper_air.to_dataset(dim="vars")
#        ds_single = darray_single_level.to_dataset(dim="vars")
#
#        # Merge datasets
#        ds_merged = xr.merge([ds_upper, ds_single])
#
#        # Add forecast_hour coordinate
#        ds_merged["forecast_hour"] = forecast_hour
#
#        # Add CF convention version
#        ds_merged.attrs["Conventions"] = "CF-1.11"
#
#        sig = signature(full_state_pressure_interpolation)
#        pres_end = sig.parameters["pres_ending"].default
#        height_end = sig.parameters["height_ending"].default
#        if "interp_pressure" in conf["predict"].keys():
#            if "surface_geopotential_var" in conf["predict"]["interp_pressure"].keys():
#                surface_geopotential_var = conf["predict"]["interp_pressure"][
#                    "surface_geopotential_var"
#                ]
#            else:
#                surface_geopotential_var = "Z_GDS4_SFC"
#            if "pres_ending" in conf["predict"]["interp_pressure"]:
#                pres_end = conf["predict"]["interp_pressure"]["pres_ending"]
#            if "height_ending" in conf["predict"]["interp_pressure"]:
#                height_end = conf["predict"]["interp_pressure"]["height_ending"]
#
#            with xr.open_dataset(conf["predict"]["static_fields"]) as static_ds:
#                surface_geopotential = static_ds[surface_geopotential_var].values
#            pressure_interp = full_state_pressure_interpolation(
#                ds_merged, surface_geopotential, **conf["predict"]["interp_pressure"]
#            )
#            ds_merged = xr.merge([ds_merged, pressure_interp])
#
#        logger.info(f"Trying to save forecast hour {forecast_hour} to {nc_filename}")
#
#        save_location = os.path.join(conf["predict"]["save_forecast"], nc_filename)
#        os.makedirs(save_location, exist_ok=True)
#
#        unique_filename = os.path.join(
#            save_location, f"pred_{nc_filename}_{forecast_hour:03d}.nc"
#        )
#        # ---------------------------------------------------- #
#        # If conf['predict']['save_vars'] provided --> drop useless vars
#        if "save_vars" in conf["predict"]:
#            if len(conf["predict"]["save_vars"]) > 0:
#                ds_merged = drop_var_from_dataset(
#                    ds_merged, conf["predict"]["save_vars"]
#                )
#
#        # when there's no metafile --> meta_data = False
#        if meta_data is not False:
#            # Add metadata attributes to every model variable if available
#            for var in ds_merged.variables.keys():
#                if var in meta_data.keys():
#                    if var != "time":
#                        # use attrs.update for non-datetime variables
#                        ds_merged[var].attrs.update(meta_data[var])
#                    else:
#                        # use time.encoding for datetime variables/coords
#                        for metadata_time in meta_data["time"]:
#                            ds_merged.time.encoding[metadata_time] = meta_data["time"][
#                                metadata_time
#                            ]
#                if "interp_pressure" in conf["predict"].keys():
#                    if pres_end in var:
#                        var_short = var.strip(pres_end)
#                        if var_short in meta_data.keys():
#                            ds_merged[var].attrs.update(meta_data[var_short])
#                            ds_merged[var].attrs["long_name"] += (
#                                " (interpolated to isobaric levels)"
#                            )
#                    elif height_end in var:
#                        var_short = var.strip(height_end)
#                        if var_short in meta_data.keys():
#                            ds_merged[var].attrs.update(meta_data[var_short])
#                            ds_merged[var].attrs["long_name"] += (
#                                " (interpolated to constant height AGL levels)"
#                            )
#        encoding_dict = {}
#        if "ua_var_encoding" in conf["predict"].keys():
#            for ua_var in conf["data"]["variables"]:
#                encoding_dict[ua_var] = conf["predict"]["ua_var_encoding"]
#        if "surface_var_encoding" in conf["predict"].keys():
#            for surface_var in conf["data"]["surface_variables"]:
#                encoding_dict[surface_var] = conf["predict"]["surface_var_encoding"]
#        if "pressure_var_encoding" in conf["predict"].keys():
#            for pres_var in conf["data"]["variables"]:
#                encoding_dict[pres_var + pres_end] = conf["predict"][
#                    "pressure_var_encoding"
#                ]
#        if "height_var_encoding" in conf["predict"].keys():
#            for height_var in conf["data"]["variables"]:
#                encoding_dict[height_var + height_end] = conf["predict"][
#                    "height_var_encoding"
#                ]
#        # Use Dask to write the dataset in parallel
#        ds_merged.to_netcdf(unique_filename, mode="w", encoding=encoding_dict)
#
#        logger.info(f"Saved forecast hour {forecast_hour} to {unique_filename}")
#    except Exception:
#        print(traceback.format_exc())
