import logging
import warnings

import torch
#import numpy as np

from credit.losses.base_losses import base_losses
# from credit.losses.spectral import SpectralLoss2D
# from credit.losses.power import PSDLoss

logger = logging.getLogger(__name__)


# todo: update me
# def variable_weights(conf, channels, frames):
#     """Create variable-specific weights for different atmospheric
#     and surface channels.
#
#     This function loads weights for different atmospheric variables
#     (e.g., U, V, T, Q) and surface variables (e.g., SP, t2m) from
#     the configuration file. It then combines them into a single
#     weight tensor for use in loss calculations.
#
#     Args:
#         conf (dict): Configuration dictionary containing the
#             variable weights.
#         channels (int): Number of channels for atmospheric variables.
#         frames (int): Number of time frames.
#
#     Returns:
#         torch.Tensor: A tensor containing the combined weights for
#             all variables.
#     """
#     # Load weights for U, V, T, Q
#     varname_upper_air = conf["data"]["variables"]
#     varname_surface = conf["data"]["surface_variables"]
#     varname_diagnostics = conf["data"]["diagnostic_variables"]
#
#     # surface + diag channels
#     N_channels_single = len(varname_surface) + len(varname_diagnostics)
#
#     weights_upper_air = torch.tensor(
#         [conf["loss"]["variable_weights"][var] for var in varname_upper_air]
#     ).view(1, channels * frames, 1, 1)
#
#     weights_single = torch.tensor(
#         [
#             conf["loss"]["variable_weights"][var]
#             for var in (varname_surface + varname_diagnostics)
#         ]
#     ).view(1, N_channels_single, 1, 1)
#
#     # Combine all weights along the color channel
#     var_weights = torch.cat([weights_upper_air, weights_single], dim=1)
#
#     return var_weights


#  Combines base loss with optional spectral and power loss
#  components.  Loss funciton can incorporate variable-specific
#  weights. [not implemented yet]


class DownscalingLoss(torch.nn.Module):
    """Custom loss function for downscaling.
    Args:
        conf (dict): configuration dictionary containing loss function
            settings and weights.
        validation (bool, optional): whether loss function is in validation
            mode.  Defaults to False.
    """

    def __init__(self, conf, validation=False):
        super(DownscalingLoss, self).__init__()

        # todo: make this a dataclass, initialize with **conf['loss']

        # todo: move this to parser
        if 'use_latitude_weights' in conf['loss']:
            warnings.warn("latitude weights not applicable to downscaling")

        for stub in ['use_variable_weights', 'use_power_loss',
                     'use_spectral_loss',]:
            if stub in conf['loss']:
                warnings.warn(f"{stub} not yet implemented for downscaling")

        self.training_loss = conf['loss']['training_loss']

        # change to fft_loss = None / 'psd' / '2d'] (radiobutton not checkbox)
        self.use_power_loss = conf['loss']['use_power_loss']
        self.use_spectral_loss = conf['loss']['use_spectral_loss']
        self.spectral_lambda_reg = conf['loss']['spectral_lambda_reg']
        self.spectral_wavenum_init = conf['loss']['spectral_wavenum_init']

        # # setup frequency-domain loss functions here:
        # self.power_loss = PSDLoss(wavenum_init)
        # self.spectral_loss = SpectralLoss2D(wavenum_init, reduction="none")

        # setup for variable-weighting; not yet implemented
        
        # you need an index that maps tensor channels to variable
        # names for this.  It exists as DownscalingDataset.tnames, but
        # we need to figure out how to pass it in to load_loss(),
        # which only gets conf as an argument

        # tnames (list): a list of the names of the variables corresponding
        #     to the channels in the output tensor generated by the model.
        #     Variable names are `dataset.variable[.zvalue]`.  This list
        #     comes from a DownscalingDataset.tnames variable.
        
        # self.tnames = tnames
        
        # self.use_variable_weights = conf['loss']['use_variable_weights']
        # if self.use_variable_weights:
        #     pass
        #     # construct tensor self.var_weights:
        #     # check that we have weights for everything in tnames
        #     # warn and/or set default values if not
        #     # create np.array from list of weights (tnames order)
        #     # convert to tensor using torch.from_numpy
        #     # self.var_weights = weights tensor

        self.validation = validation

        # for ensembles, load same loss for train and validate
        if conf["loss"]["training_loss"] == "KCRPS":  
            self.loss_fn = base_losses(conf, reduction="none", validation=False)
        elif self.validation:
            if "validation_loss" in conf["loss"]:
                self.loss_fn = base_losses(conf, reduction="none", validation=True)
            else:
                self.loss_fn = torch.nn.L1Loss(reduction="none")
        else:
            self.loss_fn = base_losses(conf, reduction="none", validation=False)


    def forward(self, target, pred):
        """Calculate the total loss for the given target and prediction.

        This method computes the base loss between the target and
        prediction, applies optional variable weights, and optionally
        adds spectral and power loss components.

        Args:
            target (torch.Tensor): Ground truth tensor.
            pred (torch.Tensor): Predicted tensor.

        Returns:
            torch.Tensor: The computed loss value.
        """

        loss = self.loss_fn(target, pred)

        # setup not yet implemented, commented code updated
        # # Variable weighting
        # if self.var_weights is not None:
        #     loss_dict = {}
        #     for i, var in enumerate(self.vars):
        #         var_loss = loss[:, i]
        #         var_loss *= self.var_weights[i].to(target.device)
        #         loss_dict[f"loss_{var}"] = var_loss.mean()
        #     
        #     loss = torch.mean(torch.stack(list(loss_dict.values())))
        # else:
        #     loss = torch.mean(loss)

        loss = torch.mean(loss)

        # not yet implemented, commented code not updated
        # # power / spectral loss
        # if not self.validation and self.use_power_loss:
        #     loss += self.power_lambda_reg * self.power_loss(
        #         target, pred, weights=self.lat_weights
        #     )
        # 
        # if not self.validation and self.use_spectral_loss:
        #     loss += (
        #         self.spectral_lambda_reg
        #         * self.spectral_loss_surface(
        #             target, pred, weights=self.lat_weights
        #         ).mean()
        #     )

        return loss
