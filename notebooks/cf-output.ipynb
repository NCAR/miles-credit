{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244eea24-3c67-466d-bddd-fbacfb068fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# ---------- #\n",
    "# Numerics\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# ---------- #\n",
    "# AI libs\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- #\n",
    "# credit\n",
    "from credit.data404 import CONUS404Dataset\n",
    "from credit.transforms404 import ToTensor, NormalizeState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4f7b7b-34ae-432e-9c48-31807c8e76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"/glade/work/mcginnis/ML/GWC/miles-credit/config/test.save.conus404.yml\"\n",
    "\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "\n",
    "conf[\"data\"][\"history_len\"]=1\n",
    "conf[\"data\"][\"forecast_len\"]=1\n",
    "\n",
    "conf[\"predict\"][\"start\"] = \"2017-11-01 00:00:00\"\n",
    "conf[\"predict\"][\"finish\"] = \"2017-11-01 23:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db4bb5a-6da6-4c9a-9262-8c701c2ed99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            # NormalizeState(conf), # uncommenting this validates that data gets transformed\n",
    "            ToTensor(conf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "ds = CONUS404Dataset(\n",
    "    varnames=conf[\"data\"][\"variables\"],\n",
    "    history_len=conf[\"data\"][\"history_len\"],\n",
    "    forecast_len=conf[\"data\"][\"forecast_len\"],\n",
    "    transform=transform,\n",
    "    start=conf[\"predict\"][\"start\"],\n",
    "    finish=conf[\"predict\"][\"finish\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562203b0-9771-4846-8994-7a56b3c107d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CONUS404Dataset(zarrpath='/glade/campaign/ral/risc/DATA/conus404/zarr', varnames=['PREC_ACC_NC', 'PSFC', 'Q2', 'T2', 'TD2'], history_len=1, forecast_len=1, transform=Compose(\n",
       "    <credit.transforms404.ToTensor object at 0x14fbb70efed0>\n",
       "), seed=22, skip_periods=None, one_shot=False, start='2017-11-01 00:00:00', finish='2017-11-01 23:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed85e008-00c5-478b-b76e-349cc37b4bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8f7328-cd9a-4f38-ba04-6c123516261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 5, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "## create a list of tensors from the input C404Dataset\n",
    "## (This is what you'd get from a model rollout on a sequence of input samples)\n",
    "\n",
    "outdims = [\"time\",\"vars\",\"z\",\"y\",\"x\"]\n",
    "\n",
    "tensorlist = []\n",
    "\n",
    "for index in range(len(ds)):\n",
    "    x = ds[index]['x'].unsqueeze(0)\n",
    "    tensorlist.append(x)\n",
    "\n",
    "outtensor = torch.cat(tensorlist, dim=0)\n",
    "print(outtensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524d65e3-fb7a-42ed-8d28-2431f0994337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 228MB\n",
      "Dimensions:                 (Time: 1, south_north: 512, west_east: 512,\n",
      "                             west_east_stag: 512, south_north_stag: 512,\n",
      "                             soil_layers_stag: 4, snow_layers_stag: 3,\n",
      "                             snso_layers_stag: 7)\n",
      "Coordinates:\n",
      "  * Time                    (Time) datetime64[ns] 8B 1979-10-01\n",
      "    XLAT                    (south_north, west_east) float32 1MB ...\n",
      "    XLONG                   (south_north, west_east) float32 1MB ...\n",
      "    XLAT_U                  (south_north, west_east_stag) float32 1MB ...\n",
      "    XLONG_U                 (south_north, west_east_stag) float32 1MB ...\n",
      "    XLAT_V                  (south_north_stag, west_east) float32 1MB ...\n",
      "    XLONG_V                 (south_north_stag, west_east) float32 1MB ...\n",
      "    XTIME                   (Time) datetime64[ns] 8B ...\n",
      "Dimensions without coordinates: south_north, west_east, west_east_stag,\n",
      "                                south_north_stag, soil_layers_stag,\n",
      "                                snow_layers_stag, snso_layers_stag\n",
      "Data variables: (12/194)\n",
      "    ACDEWC                  (Time, south_north, west_east) float32 1MB ...\n",
      "    ACDRIPR                 (Time, south_north, west_east) float32 1MB ...\n",
      "    ACDRIPS                 (Time, south_north, west_east) float32 1MB ...\n",
      "    ACECAN                  (Time, south_north, west_east) float32 1MB ...\n",
      "    ACEDIR                  (Time, south_north, west_east) float32 1MB ...\n",
      "    ACETLSM                 (Time, south_north, west_east) float32 1MB ...\n",
      "    ...                      ...\n",
      "    totalIce                (Time, south_north, west_east) float32 1MB ...\n",
      "    totalLiq                (Time, south_north, west_east) float32 1MB ...\n",
      "    totalVap                (Time, south_north, west_east) float32 1MB ...\n",
      "    index_snow_layers_stag  (snow_layers_stag) int32 12B ...\n",
      "    index_snso_layers_stag  (snso_layers_stag) int32 28B ...\n",
      "    index_soil_layers_stag  (soil_layers_stag) int32 16B ...\n",
      "Attributes: (12/151)\n",
      "    TITLE:                            OUTPUT FROM WRF V3.9.1.1 MODEL\n",
      "    START_DATE:                      1979-10-01_00:00:00\n",
      "    SIMULATION_START_DATE:           1979-10-01_00:00:00\n",
      "    WEST-EAST_GRID_DIMENSION:        1368\n",
      "    SOUTH-NORTH_GRID_DIMENSION:      1016\n",
      "    BOTTOM-TOP_GRID_DIMENSION:       51\n",
      "    ...                              ...\n",
      "    Source_List:                     ./wrf_constants_list_conus404_v20201005.csv\n",
      "    history:                         Mon Sep 12 21:16:06 2022: ncks -v XLAT_V...\n",
      "    Source_Code:                     make_conusii_2d.csh\n",
      "    Project:                         USGS404\n",
      "    Conventions:                     CF-1.0\n",
      "    Usage:                           ncgen -k nc4 -o 3_layers_stag.nc 3_layer...\n"
     ]
    }
   ],
   "source": [
    "## open template file\n",
    "c404_path = \"/glade/campaign/collections/rda/data/ds559.0/\"\n",
    "template_path = \"wy1980/197910/wrf2d_d01_1979-10-01_00:00:00.nc\"\n",
    "# template_path = \"wy1980/197910/wrf3d_d01_1979-10-01_00:00:00.nc\"\n",
    "raw_template = xr.open_dataset(c404_path + template_path)\n",
    "\n",
    "## subset to region (needs to move from transforms404 to config)\n",
    "x0 = 120\n",
    "xsize = 512\n",
    "y0 = 300\n",
    "ysize = 512\n",
    "\n",
    "xs = slice(x0, x0+xsize)\n",
    "ys = slice(y0, y0+ysize)\n",
    "\n",
    "template = raw_template.isel(south_north=ys, south_north_stag=ys, west_east=xs, west_east_stag=xs)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26af16af-e4dd-4640-9685-cf43564b5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert output tensor to list of xarray dataarrays\n",
    "\n",
    "## We have to split it into separate xarrays for each variable because CONUS404 still has winds\n",
    "## on staggered grids, and to attach the 3 different sets of 2D lat/lon coordinates appropriately,\n",
    "## they can't all be merged into the same dataset.\n",
    "\n",
    "## HOWEVER, this is only for 3-D wind variables; not surface vars. BUT, for 3-D vars I'd have to \n",
    "## split the varnames back into var + hPa-to-level, and I think the right thing to do there is to\n",
    "## follow what the new data pipeline is doing and leave the 3D vars as 3D vars instead of pulling\n",
    "## out the levels we want, so I'd either need to change my zarrification workflow and redo all of\n",
    "## those, or (better) (hopefully) just throw some kerchunk / virtualiZarr indexing onto the raw\n",
    "## CONUS404 netcdf files and not actually zarrify them at all.  And that's still a ways down the\n",
    "## road, so the multiple 2D lat/lon for different staggers is completely untested at this point.\n",
    "\n",
    "xrlist = []\n",
    "\n",
    "## Loop on output variables:\n",
    "\n",
    "vnames = conf[\"data\"][\"variables\"]\n",
    "\n",
    "for i in range(len(vnames)):\n",
    "    v = vnames[i]\n",
    "    \n",
    "    ## Pull corresponding slice from tensor\n",
    "    ## tensor dimensions: time, var, z, y, x\n",
    "    ## squeeze to drop singleton z dim\n",
    "    vdata = outtensor[:,i,:,:,:].squeeze()  \n",
    "\n",
    "    ## convert to xarray & name dims according to template\n",
    "    vxra = xr.DataArray(vdata, dims=template[v].dims)\n",
    "\n",
    "    ## copy metadata\n",
    "    vxra.attrs = template[v].attrs\n",
    "\n",
    "    ## copy coords -- first, we need to drop time coordinates\n",
    "    tvc = template[v].coords\n",
    "    ckeep = [cname for cname in list(tvc) if \"time\" not in cname.lower()]\n",
    "    ## then create a dictionary of those coordinates\n",
    "    #cdict = {k: tvc[k] for k in ckeep}\n",
    "    cdict = {}\n",
    "    for k in ckeep:\n",
    "        coord = tvc[k]\n",
    "        for L in (\"latitude\", \"longitude\"):\n",
    "            if L in coord.attrs[\"description\"].lower():\n",
    "                coord.attrs[\"standard_name\"] = L\n",
    "        cdict[k] = coord\n",
    "\n",
    "    ## and now we can assign those coordinates to the dataarray\n",
    "    vxra = vxra.assign_coords(cdict)\n",
    "    \n",
    "    ## add  to list\n",
    "    xrlist.append({v: vxra})\n",
    "\n",
    "#print(xrlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df508279-489d-4284-9632-29a18724d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine list into dataset\n",
    "dsout = xr.merge(xrlist)\n",
    "\n",
    "## WRF time coordinate is missing attributes, create it by hand\n",
    "nt = outtensor.size()[0]  ## time is first dimension\n",
    "timevals = [float(t) for t in range(nt)]\n",
    "timeatts = {\n",
    "    \"standard_name\": \"time\",\n",
    "    \"long_name\": \"time\",\n",
    "    \"units\": \"hours since \" + conf[\"predict\"][\"start\"],\n",
    "    \"calendar\": \"proleptic_gregorian\"\n",
    "}\n",
    "time = xr.DataArray(data=timevals, dims=ds.tdimname, attrs=timeatts)\n",
    "dsout = dsout.assign_coords({ds.tdimname: time})\n",
    "\n",
    "## copy / write global metadata\n",
    "\n",
    "dsout.attrs[\"Conventions\"] = \"CF-1.11\"\n",
    "dsout.attrs[\"frequency\"] = \"1hr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80468ddc-3878-4f6c-90fb-d7c5f6b12496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add map projection \n",
    "\n",
    "tatt = template.attrs\n",
    "\n",
    "if tatt[\"MAP_PROJ_CHAR\"] != \"Lambert Conformal\":\n",
    "    raise ValueError(\"WRF map projection is not Lambert Conformal; don't know how to deal with others yet\")\n",
    "\n",
    "proj = xr.DataArray(None, attrs={\n",
    "    \"grid_mapping_name\": \"lambert_conformal_conic\",\n",
    "    \"earth_radius\": float(6370000),\n",
    "    \"standard_parallel\": [tatt[\"TRUELAT1\"], tatt[\"TRUELAT2\"]],\n",
    "    \"longitude_of_central_meridian\": tatt[\"STAND_LON\"],\n",
    "    \"latitude_of_projection_origin\": tatt[\"MOAD_CEN_LAT\"]\n",
    "})\n",
    "\n",
    "pname = \"LCC\"\n",
    "\n",
    "dsout[pname] = proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bc0654-8c71-4317-b6c7-ac0d5d0a1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard names\n",
    "\n",
    "stdname = {\n",
    "    # \"ACSWDNLSM\": \"\", divide by 60 minutes to get downwelling_shortwave_flux_in_air\n",
    "    # \"COSZEN\": \"\",  # solar_zenith_angle exists, but not cosine thereof\n",
    "    \"PREC_ACC_NC\": \"lwe_thickness_of_precipitation_amount\",\n",
    "    # CONUS404 is convection-resolving, so grid-scale precip is all of it\n",
    "    # units of length makes it lwe (liquid water equivalent)\n",
    "    \"PSFC\": \"surface_air_pressure\",\n",
    "    \"Q2\": \"humidity_mixing_ratio\",\n",
    "    \"SNOW\": \"snowfall_amount\",\n",
    "    \"TD2\": \"dew_point_temperature\",\n",
    "    \"T2\": \"air_temperature\",\n",
    "    \"totalVap\": \"atmosphere_mass_content_of_water_vapor\",\n",
    "}\n",
    "\n",
    "for v in vnames:\n",
    "    if v in stdname:\n",
    "        dsout[v].attrs[\"standard_name\"] = stdname[v]\n",
    "    if v in (\"T2\"):  # for temperatures, need to indicate absolute vs delta\n",
    "        dsout[v].attrs[\"units_metadata\"] = \"on_scale\"\n",
    "\n",
    "# Possible TODO: sort attributes alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1992cbd-7ff0-4a1c-8599-dcf9508254de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: add coordinate values (1:N x 4) to x & y axes, units = 'km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c357ba-3dfe-4f02-81c0-cf640482bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: add scalar 2-m height coordinate for appropriate variables\n",
    "## it appears this needs to be done using xarray coordinates, not just appending to attribute\n",
    "\n",
    "two_meter = (\"Q2\", \"TD2\", \"T2\")  # vars that need 2-m scalar height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e142a5fa-8d09-436d-aebb-05973360be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to netcdf\n",
    "\n",
    "commonatts = {\"_FillValue\": 1e20,\n",
    "              \"missing_value\": 1e20,\n",
    "              \"grid_mapping\": \"LCC\"}\n",
    "\n",
    "encodict = {v: commonatts for v in vnames}\n",
    "\n",
    "dsout.to_netcdf(\"/glade/work/mcginnis/ML/GWC/format-test.nc\",\n",
    "               unlimited_dims = ds.tdimname,\n",
    "               encoding = encodict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce357b-b552-445d-874b-b6ceb575830d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
