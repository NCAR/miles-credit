{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5b32b4-9512-42bf-b712-d9c997035fe0",
   "metadata": {},
   "source": [
    "# Develop and test Pytorch IterableDataset for multi-step training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4343d9ac-a628-4c0c-8254-214b901c172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import tqdm\n",
    "\n",
    "import random\n",
    "#import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d502698b-7e20-4b6d-8f70-08bccde7ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credit.transforms import load_transforms\n",
    "from credit.data import Sample, drop_var_from_dataset, get_forward_data_netCDF4, find_key_for_number, extract_month_day_hour, find_common_indices, ERA5_and_Forcing_Dataset, get_forward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9192aa9-f968-4a0e-9e87-28c92a30c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "\n",
    "with open('/glade/derecho/scratch/schreck/repos/miles-credit/results/wxformer/6hr/model.yml') as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc27c7-216d-4ed1-911f-baf56ce14082",
   "metadata": {},
   "source": [
    "## Load transforms and single-step / one-shot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f43baf7-d7c9-410c-9036-fa2f9a04e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "varname_upper_air = conf['data']['variables']\n",
    "surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "diagnostic_files = None # sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "is_train = False\n",
    "\n",
    "if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "    forcing_files = conf['data']['save_loc_forcing']\n",
    "    varname_forcing = conf['data']['forcing_variables']\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "\n",
    "if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "    static_files = conf['data']['save_loc_static']\n",
    "    varname_static = conf['data']['static_variables']\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None\n",
    "\n",
    "if surface_files is not None:\n",
    "    varname_surface = conf['data']['surface_variables']\n",
    "else:\n",
    "    varname_surface = None\n",
    "\n",
    "if diagnostic_files is not None:\n",
    "    varname_diagnostic = conf['data']['diagnostic_variables']\n",
    "else:\n",
    "    varname_diagnostic = None\n",
    "\n",
    "# number of previous lead time inputs\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "# number of lead times to forecast\n",
    "forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "\n",
    "if is_train:\n",
    "    history_len = history_len\n",
    "    forecast_len = forecast_len\n",
    "    # print out training / validation\n",
    "    name = \"training\"\n",
    "else:\n",
    "    history_len = valid_history_len\n",
    "    forecast_len = valid_forecast_len\n",
    "    name = 'validation'\n",
    "\n",
    "# max_forecast_len\n",
    "if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "    max_forecast_len = None\n",
    "else:\n",
    "    max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "# skip_periods\n",
    "if \"skip_periods\" not in conf[\"data\"]:\n",
    "    skip_periods = None\n",
    "else:\n",
    "    skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "\n",
    "# one_shot\n",
    "if \"one_shot\" not in conf[\"data\"]:\n",
    "    one_shot = None\n",
    "else:\n",
    "    one_shot = conf[\"data\"][\"one_shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05581783-78fd-4737-8b9c-41055ecf3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing utils\n",
    "transforms = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05e66e2-a9a9-4099-af0f-35610bf86c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_Dataset(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=one_shot,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714ddbdb-934a-4ed7-a249-a46105019a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5de8bb2-e509-47cc-aee8-c44aa2f53e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_forcing_static', 'x_surf', 'x', 'y_surf', 'y', 'index'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d15b09-f7a2-4e90-a8cd-0e099ec1a0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58019f2a-1918-4d7d-8e95-12191c17b97e",
   "metadata": {},
   "source": [
    "## Load N multi-step dataset where N is seqeunce length (Noah's loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a9edfd-d0f2-446a-a392-45ca1111a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedSequentialDatasetV2(torch.utils.data.IterableDataset):\n",
    "    '''\n",
    "    torch.utils.data.IterableDataset for multi-step training\n",
    "\n",
    "    Reference:\n",
    "    https://colab.research.google.com/drive/1OFLZnX9y5QUFNONuvFsxOizq4M-tFvk-?usp=sharing#scrollTo=CxSCQPOMHgwo\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None,\n",
    "        shuffle=True\n",
    "    ):\n",
    "        \n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times \n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "        \n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # ERA5 operations\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "        \n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "            \n",
    "        self.all_files = all_files\n",
    "        \n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {} # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [len(ERA5_xarray['time']),\n",
    "                                                ind_start,\n",
    "                                                ind_start + len(ERA5_xarray['time'])]\n",
    "            ind_start += len(ERA5_xarray['time']) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(filename_forcing), 'Cannot find forcing file [{}]'.format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data_netCDF4(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "            \n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(filename_static), 'Cannot find static file [{}]'.format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data_netCDF4(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "            \n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "        \n",
    "        if self.filename_diagnostic is not None:\n",
    "\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "            \n",
    "            for fn in filename_diagnostic:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_diagnostic)\n",
    "                \n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "                \n",
    "            self.diagnostic_files = diagnostic_files\n",
    "            \n",
    "            assert len(self.diagnostic_files)==len(self.all_files), \\\n",
    "                'Mismatch between the total number of diagnostic files and upper-air files'\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "            \n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "        \n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "        \n",
    "            for fn in filename_surface:\n",
    "                \n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "                \n",
    "                surface_files.append(xarray_dataset)\n",
    "                \n",
    "            self.surface_files = surface_files\n",
    "            \n",
    "            assert len(self.surface_files)==len(self.all_files), \\\n",
    "                'Mismatch between the total number of surface files and upper-air files'\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray['time']) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        worker_info = get_worker_info()\n",
    "        \n",
    "        num_workers = worker_info.num_workers if worker_info is not None else 1\n",
    "        \n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        \n",
    "        sampler = DistributedSampler(self, num_replicas=num_workers * self.world_size,\n",
    "                                     rank=self.rank * num_workers + worker_id, shuffle=self.shuffle)\n",
    "        \n",
    "        sampler.set_epoch(self.current_epoch)\n",
    "    \n",
    "        for index in iter(sampler):\n",
    "\n",
    "            # get a range of ind_start for different lead time (multi forecast steps)\n",
    "            indices = list(range(index, index + self.history_len + self.forecast_len))\n",
    "            stop_forecast = False\n",
    "    \n",
    "            for k, ind_start_current_step in enumerate(indices):\n",
    "    \n",
    "                # select the ind_file based on the iter index \n",
    "                ind_file = find_key_for_number(ind_start_current_step, self.ERA5_indices)\n",
    "        \n",
    "                # get the ind within the current file\n",
    "                ind_start = self.ERA5_indices[ind_file][1]\n",
    "                ind_start_in_file = ind_start_current_step - ind_start\n",
    "        \n",
    "                # handle out-of-bounds\n",
    "                ind_largest = len(self.all_files[int(ind_file)]['time'])-(self.history_len+self.forecast_len+1)\n",
    "                if ind_start_in_file > ind_largest:\n",
    "                    ind_start_in_file = ind_largest\n",
    "                    \n",
    "                # ========================================================================== #\n",
    "                # subset xarray on time dimension & load it to the memory\n",
    "                \n",
    "                ind_end_in_file = ind_start_in_file+self.history_len+self.forecast_len\n",
    "                \n",
    "                ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "                ERA5_subset = self.all_files[int(ind_file)].isel(\n",
    "                    time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "                \n",
    "                if self.surface_files:\n",
    "                    ## subset surface variables\n",
    "                    surface_subset = self.surface_files[int(ind_file)].isel(\n",
    "                        time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "                    \n",
    "                    ## merge upper-air and surface here:\n",
    "                    ERA5_subset = ERA5_subset.merge(surface_subset) # <-- lazy merge, ERA5 and surface both not loaded\n",
    "        \n",
    "                # ==================================================== #\n",
    "                # split ERA5_subset into training inputs and targets\n",
    "                #   + merge with forcing and static\n",
    "        \n",
    "                # the ind_end of the ERA5_subset\n",
    "                ind_end_time = len(ERA5_subset['time'])\n",
    "        \n",
    "                # datetiem information as int number (used in some normalization methods)\n",
    "                datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "        \n",
    "                # ==================================================== #\n",
    "                # xarray dataset as input\n",
    "                ## historical_ERA5_images: the final input\n",
    "        \n",
    "                historical_ERA5_images = ERA5_subset.isel(\n",
    "                    time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "                # merge forcing inputs\n",
    "                if self.xarray_forcing:\n",
    "                    # =============================================================================== #\n",
    "                    # matching month, day, hour between forcing and upper air [time]\n",
    "                    # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "                    month_day_forcing = extract_month_day_hour(np.array(self.xarray_forcing['time']))\n",
    "                    month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time'])) # <-- upper air\n",
    "                    # indices to subset\n",
    "                    ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "                    forcing_subset_input = self.xarray_forcing.isel(time=ind_forcing).load() # <-- load into memory\n",
    "                    # forcing and upper air have different years but the same mon/day/hour\n",
    "                    # safely replace forcing time with upper air time\n",
    "                    forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "                    # =============================================================================== #\n",
    "        \n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "        \n",
    "                # merge static inputs\n",
    "                if self.xarray_static:\n",
    "                    # expand static var on time dim\n",
    "                    N_time_dims = len(ERA5_subset['time'])\n",
    "                    static_subset_input = self.xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "                    # assign coords 'time'\n",
    "                    static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "        \n",
    "                    # slice + load to the GPU\n",
    "                    static_subset_input = static_subset_input.isel(\n",
    "                        time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "                    # update \n",
    "                    static_subset_input['time'] = historical_ERA5_images['time']\n",
    "        \n",
    "                    # merge\n",
    "                    historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "                \n",
    "                # ==================================================== #\n",
    "                # xarray dataset as target\n",
    "                ## target_ERA5_images: the final target\n",
    "        \n",
    "                if self.one_shot is not None:\n",
    "                    # one_shot is True (on), go straight to the last element\n",
    "                    target_ERA5_images = ERA5_subset.isel(time=slice(-1, None)).load() # <-- load into memory\n",
    "                    \n",
    "                    ## merge diagnoisc input here:\n",
    "                    if self.diagnostic_files:\n",
    "                        diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                            time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "                        \n",
    "                        diagnostic_subset = diagnostic_subset.isel(\n",
    "                            time=slice(-1, None)).load() # <-- load into memory\n",
    "                        \n",
    "                        target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "                        \n",
    "                else:\n",
    "                    # one_shot is None (off), get the full target length based on forecast_len\n",
    "                    target_ERA5_images = ERA5_subset.isel(\n",
    "                        time=slice(self.history_len, ind_end_time, self.skip_periods)).load() # <-- load into memory\n",
    "            \n",
    "                    ## merge diagnoisc input here:\n",
    "                    if self.diagnostic_files:\n",
    "                        \n",
    "                        # subset diagnostic variables\n",
    "                        diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                            time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "                        \n",
    "                        diagnostic_subset = diagnostic_subset.isel(\n",
    "                            time=slice(self.history_len, ind_end_time, self.skip_periods)).load() # <-- load into memory\n",
    "                        \n",
    "                        # merge into the target dataset\n",
    "                        target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "        \n",
    "                # pipe xarray datasets to the sampler\n",
    "                sample = Sample(\n",
    "                    historical_ERA5_images=historical_ERA5_images,\n",
    "                    target_ERA5_images=target_ERA5_images,\n",
    "                    datetime_index=datetime_as_number\n",
    "                )\n",
    "        \n",
    "                # ==================================== #\n",
    "                # data normalization\n",
    "                if self.transform:\n",
    "                    sample = self.transform(sample)\n",
    "\n",
    "                # assign sample index\n",
    "                sample[\"index\"] = index\n",
    "\n",
    "                # k: current forecast step\n",
    "                # if k == the last forecast step --> stop\n",
    "                stop_forecast = (k == self.forecast_len)\n",
    "                \n",
    "                sample['forecast_hour'] = k\n",
    "                sample['index'] = index\n",
    "                sample['stop_forecast'] = stop_forecast\n",
    "                sample[\"datetime\"] = [\n",
    "                    int(historical_ERA5_images.time.values[0].astype('datetime64[s]').astype(int)),\n",
    "                    int(target_ERA5_images.time.values[0].astype('datetime64[s]').astype(int))\n",
    "                ]\n",
    "\n",
    "                # print out to check input and target datetimes\n",
    "                print('Input time: {}'.format(np.array(historical_ERA5_images['time'])))\n",
    "                print('Target time: {}'.format(np.array(target_ERA5_images['time'])))\n",
    "                \n",
    "                yield sample\n",
    "    \n",
    "                if stop_forecast:\n",
    "                    break\n",
    "    \n",
    "                if (k == self.forecast_len):\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81e06e-857a-49ac-8fd1-994a5933ffec",
   "metadata": {},
   "source": [
    "### one-shot with printed datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852dfacc-2650-4fa9-9122-4e5c16512125",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 3 # really its 4\n",
    "\n",
    "# Z-score\n",
    "dataset = DistributedSequentialDatasetV2(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=True,  # <------------------------------------- !! None = no one_shot, True = one_shot\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9fddcd-d67d-4706-8ef0-9c1df50f5884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input time: ['1979-01-01T00:00:00.000000000']\n",
      "Target time: ['1979-01-02T00:00:00.000000000']\n",
      "0 False\n",
      "Input time: ['1979-01-01T06:00:00.000000000']\n",
      "Target time: ['1979-01-02T06:00:00.000000000']\n",
      "1 False\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-02T12:00:00.000000000']\n",
      "2 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T18:00:00.000000000']\n",
      "3 True\n",
      "Input time: ['1979-01-01T06:00:00.000000000']\n",
      "Target time: ['1979-01-02T06:00:00.000000000']\n",
      "4 False\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-02T12:00:00.000000000']\n",
      "5 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T18:00:00.000000000']\n",
      "6 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-03T00:00:00.000000000']\n",
      "7 True\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-02T12:00:00.000000000']\n",
      "8 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T18:00:00.000000000']\n",
      "9 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-03T00:00:00.000000000']\n",
      "10 False\n",
      "Input time: ['1979-01-02T06:00:00.000000000']\n",
      "Target time: ['1979-01-03T06:00:00.000000000']\n",
      "11 True\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T18:00:00.000000000']\n",
      "12 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-03T00:00:00.000000000']\n",
      "13 False\n",
      "Input time: ['1979-01-02T06:00:00.000000000']\n",
      "Target time: ['1979-01-03T06:00:00.000000000']\n",
      "14 False\n",
      "Input time: ['1979-01-02T12:00:00.000000000']\n",
      "Target time: ['1979-01-03T12:00:00.000000000']\n",
      "15 True\n"
     ]
    }
   ],
   "source": [
    "for k, result in enumerate(dataset):\n",
    "    print(k, result['stop_forecast'])\n",
    "    if (k + 1) == forecast_len * 5 + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aab014-ef51-4203-9f71-f5a9db655953",
   "metadata": {},
   "source": [
    "### Noah with printed datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff114d6-0ac4-4d30-b2df-ff2b28d1025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_len = 3 # really its 4\n",
    "\n",
    "# Z-score\n",
    "dataset = DistributedSequentialDatasetV2(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=None,  # <------------------------------------- !! None = no one_shot, True = one_shot\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a819b4b0-dec5-4a0d-9581-3b3e1969cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input time: ['1979-01-01T00:00:00.000000000']\n",
      "Target time: ['1979-01-01T06:00:00.000000000' '1979-01-01T12:00:00.000000000'\n",
      " '1979-01-01T18:00:00.000000000' '1979-01-02T00:00:00.000000000']\n",
      "0 False\n",
      "Input time: ['1979-01-01T06:00:00.000000000']\n",
      "Target time: ['1979-01-01T12:00:00.000000000' '1979-01-01T18:00:00.000000000'\n",
      " '1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000']\n",
      "1 False\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-01T18:00:00.000000000' '1979-01-02T00:00:00.000000000'\n",
      " '1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000']\n",
      "2 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000'\n",
      " '1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000']\n",
      "3 True\n",
      "Input time: ['1979-01-01T06:00:00.000000000']\n",
      "Target time: ['1979-01-01T12:00:00.000000000' '1979-01-01T18:00:00.000000000'\n",
      " '1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000']\n",
      "4 False\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-01T18:00:00.000000000' '1979-01-02T00:00:00.000000000'\n",
      " '1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000']\n",
      "5 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000'\n",
      " '1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000']\n",
      "6 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000'\n",
      " '1979-01-02T18:00:00.000000000' '1979-01-03T00:00:00.000000000']\n",
      "7 True\n",
      "Input time: ['1979-01-01T12:00:00.000000000']\n",
      "Target time: ['1979-01-01T18:00:00.000000000' '1979-01-02T00:00:00.000000000'\n",
      " '1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000']\n",
      "8 False\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000'\n",
      " '1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000']\n",
      "9 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000'\n",
      " '1979-01-02T18:00:00.000000000' '1979-01-03T00:00:00.000000000']\n",
      "10 False\n",
      "Input time: ['1979-01-02T06:00:00.000000000']\n",
      "Target time: ['1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000'\n",
      " '1979-01-03T00:00:00.000000000' '1979-01-03T06:00:00.000000000']\n",
      "11 True\n",
      "Input time: ['1979-01-01T18:00:00.000000000']\n",
      "Target time: ['1979-01-02T00:00:00.000000000' '1979-01-02T06:00:00.000000000'\n",
      " '1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000']\n",
      "12 False\n",
      "Input time: ['1979-01-02T00:00:00.000000000']\n",
      "Target time: ['1979-01-02T06:00:00.000000000' '1979-01-02T12:00:00.000000000'\n",
      " '1979-01-02T18:00:00.000000000' '1979-01-03T00:00:00.000000000']\n",
      "13 False\n",
      "Input time: ['1979-01-02T06:00:00.000000000']\n",
      "Target time: ['1979-01-02T12:00:00.000000000' '1979-01-02T18:00:00.000000000'\n",
      " '1979-01-03T00:00:00.000000000' '1979-01-03T06:00:00.000000000']\n",
      "14 False\n",
      "Input time: ['1979-01-02T12:00:00.000000000']\n",
      "Target time: ['1979-01-02T18:00:00.000000000' '1979-01-03T00:00:00.000000000'\n",
      " '1979-01-03T06:00:00.000000000' '1979-01-03T12:00:00.000000000']\n",
      "15 True\n"
     ]
    }
   ],
   "source": [
    "for k, result in enumerate(dataset):\n",
    "    print(k, result['stop_forecast'])\n",
    "    if (k + 1) == forecast_len * 5 + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc76636-415e-42b8-913b-7b79ba22f217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
