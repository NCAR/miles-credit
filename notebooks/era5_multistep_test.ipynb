{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b573c33-7046-4d9f-9a0f-cbc44e562adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from credit.data import *\n",
    "from credit.transforms import load_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import get_worker_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836fdbf5-906a-4b80-afab-02e96aabc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/glade/derecho/scratch/schreck/repos/miles-credit/production/multistep/wxformer_6h/model.yml') as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dbc422-abae-40a2-ac72-a87f37eab616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "all_ERA_files = sorted(glob.glob(conf[\"data\"][\"save_loc\"]))\n",
    "\n",
    "# <------------------------------------------ std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "\n",
    "    # check and glob surface files\n",
    "    if ('surface_variables' in conf['data']) and (len(conf['data']['surface_variables']) > 0):\n",
    "        surface_files = sorted(glob.glob(conf[\"data\"][\"save_loc_surface\"]))\n",
    "        \n",
    "    else:\n",
    "        surface_files = None\n",
    "\n",
    "    # check and glob dyn forcing files\n",
    "    if ('dynamic_forcing_variables' in conf['data']) and (len(conf['data']['dynamic_forcing_variables']) > 0):\n",
    "        dyn_forcing_files = sorted(glob.glob(conf[\"data\"][\"save_loc_dynamic_forcing\"]))\n",
    "        \n",
    "    else:\n",
    "        dyn_forcing_files = None\n",
    "\n",
    "    # check and glob diagnostic files\n",
    "    if ('diagnostic_variables' in conf['data']) and (len(conf['data']['diagnostic_variables']) > 0):\n",
    "        diagnostic_files = sorted(glob.glob(conf[\"data\"][\"save_loc_diagnostic\"]))\n",
    "        \n",
    "    else:\n",
    "        diagnostic_files = None\n",
    "\n",
    "# -------------------------------------------------- #\n",
    "# import training / validation years from conf\n",
    "\n",
    "if 'train_years' in conf['data']:\n",
    "    train_years_range = conf['data']['train_years']\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if 'valid_years' in conf['data']:\n",
    "    valid_years_range = conf['data']['valid_years']\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [file for file in all_ERA_files if any(year in file for year in train_years)]\n",
    "valid_files = [file for file in all_ERA_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    \n",
    "    if surface_files is not None:\n",
    "        \n",
    "        train_surface_files = [file for file in surface_files if any(year in file for year in train_years)]\n",
    "        valid_surface_files = [file for file in surface_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_surface_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [surface files] and [upper-air files]'\n",
    "        assert len(valid_surface_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [surface files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_surface_files = None\n",
    "        valid_surface_files = None\n",
    "\n",
    "    if dyn_forcing_files is not None:\n",
    "        \n",
    "        train_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in train_years)]\n",
    "        valid_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_dyn_forcing_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [dynamic forcing files] and [upper-air files]'\n",
    "        assert len(valid_dyn_forcing_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [dynamic forcing files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_dyn_forcing_files = None\n",
    "        valid_dyn_forcing_files = None\n",
    "        \n",
    "    if diagnostic_files is not None:\n",
    "        \n",
    "        train_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in train_years)]\n",
    "        valid_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_diagnostic_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [diagnostic files] and [upper-air files]'\n",
    "        assert len(valid_diagnostic_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [diagnostic files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_diagnostic_files = None\n",
    "        valid_diagnostic_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb14e2b-f1a7-460b-b8dd-599decad7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert $USER to the actual user name\n",
    "conf['save_loc'] = os.path.expandvars(conf['save_loc'])\n",
    "\n",
    "# ======================================================== #\n",
    "# parse intputs\n",
    "\n",
    "# upper air variables\n",
    "varname_upper_air = conf['data']['variables']\n",
    "\n",
    "if ('forcing_variables' in conf['data']) and (len(conf['data']['forcing_variables']) > 0):\n",
    "    forcing_files = conf['data']['save_loc_forcing']\n",
    "    varname_forcing = conf['data']['forcing_variables']\n",
    "else:\n",
    "    forcing_files = None\n",
    "    varname_forcing = None\n",
    "    \n",
    "if ('static_variables' in conf['data']) and (len(conf['data']['static_variables']) > 0):\n",
    "    static_files = conf['data']['save_loc_static']\n",
    "    varname_static = conf['data']['static_variables']\n",
    "else:\n",
    "    static_files = None\n",
    "    varname_static = None\n",
    "\n",
    "# get surface variable names\n",
    "if surface_files is not None:\n",
    "    varname_surface = conf['data']['surface_variables']\n",
    "else:\n",
    "    varname_surface = None\n",
    "\n",
    "# get dynamic forcing variable names\n",
    "if dyn_forcing_files is not None:\n",
    "    varname_dyn_forcing = conf['data']['dynamic_forcing_variables']\n",
    "else:\n",
    "    varname_dyn_forcing = None\n",
    "\n",
    "# get diagnostic variable names\n",
    "if diagnostic_files is not None:\n",
    "    varname_diagnostic = conf['data']['diagnostic_variables']\n",
    "else:\n",
    "    varname_diagnostic = None\n",
    "        \n",
    "# number of previous lead time inputs\n",
    "history_len = conf[\"data\"][\"history_len\"]\n",
    "valid_history_len = conf[\"data\"][\"valid_history_len\"]\n",
    "\n",
    "# number of lead times to forecast\n",
    "forecast_len = conf[\"data\"][\"forecast_len\"]\n",
    "valid_forecast_len = conf[\"data\"][\"valid_forecast_len\"]\n",
    "    \n",
    "# max_forecast_len\n",
    "if \"max_forecast_len\" not in conf[\"data\"]:\n",
    "    max_forecast_len = None\n",
    "else:\n",
    "    max_forecast_len = conf[\"data\"][\"max_forecast_len\"]\n",
    "\n",
    "# skip_periods\n",
    "if \"skip_periods\" not in conf[\"data\"]:\n",
    "    skip_periods = None\n",
    "else:\n",
    "    skip_periods = conf[\"data\"][\"skip_periods\"]\n",
    "    \n",
    "# one_shot\n",
    "if \"one_shot\" not in conf[\"data\"]:\n",
    "    one_shot = None\n",
    "else:\n",
    "    one_shot = conf[\"data\"][\"one_shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceaa80b3-75c7-4a47-8914-59179c27ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_years' in conf['data']:\n",
    "    train_years_range = conf['data']['train_years']\n",
    "else:\n",
    "    train_years_range = [1979, 2014]\n",
    "\n",
    "if 'valid_years' in conf['data']:\n",
    "    valid_years_range = conf['data']['valid_years']\n",
    "else:\n",
    "    valid_years_range = [2014, 2018]\n",
    "\n",
    "# convert year info to str for file name search\n",
    "train_years = [str(year) for year in range(train_years_range[0], train_years_range[1])]\n",
    "valid_years = [str(year) for year in range(valid_years_range[0], valid_years_range[1])]\n",
    "\n",
    "# Filter the files for training / validation\n",
    "train_files = [file for file in all_ERA_files if any(year in file for year in train_years)]\n",
    "valid_files = [file for file in all_ERA_files if any(year in file for year in valid_years)]\n",
    "\n",
    "# <----------------------------------- std_new\n",
    "if conf['data']['scaler_type'] == 'std_new':\n",
    "    \n",
    "    if surface_files is not None:\n",
    "        \n",
    "        train_surface_files = [file for file in surface_files if any(year in file for year in train_years)]\n",
    "        valid_surface_files = [file for file in surface_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_surface_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [surface files] and [upper-air files]'\n",
    "        assert len(valid_surface_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [surface files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_surface_files = None\n",
    "        valid_surface_files = None\n",
    "\n",
    "    if dyn_forcing_files is not None:\n",
    "        \n",
    "        train_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in train_years)]\n",
    "        valid_dyn_forcing_files = [file for file in dyn_forcing_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_dyn_forcing_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [dynamic forcing files] and [upper-air files]'\n",
    "        assert len(valid_dyn_forcing_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [dynamic forcing files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_dyn_forcing_files = None\n",
    "        valid_dyn_forcing_files = None\n",
    "        \n",
    "    if diagnostic_files is not None:\n",
    "        \n",
    "        train_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in train_years)]\n",
    "        valid_diagnostic_files = [file for file in diagnostic_files if any(year in file for year in valid_years)]\n",
    "\n",
    "        # ---------------------------- #\n",
    "        # check total number of files\n",
    "        assert len(train_diagnostic_files) == len(train_files), \\\n",
    "        'Mismatch between the total number of training set [diagnostic files] and [upper-air files]'\n",
    "        assert len(valid_diagnostic_files) == len(valid_files), \\\n",
    "        'Mismatch between the total number of validation set [diagnostic files] and [upper-air files]'\n",
    "    \n",
    "    else:\n",
    "        train_diagnostic_files = None\n",
    "        valid_diagnostic_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba800a1-2acc-4bab-b89e-656379353d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/conda-envs/credit/lib/python3.11/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "transforms = load_transforms(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b70e456-e30f-4def-8fdd-3269862e8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5_and_Forcing_MultiStep(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class that works on:\n",
    "        - upper-air variables (time, level, lat, lon)\n",
    "        - surface variables (time, lat, lon)\n",
    "        - dynamic forcing variables (time, lat, lon)\n",
    "        - foring variables (time, lat, lon)\n",
    "        - diagnostic variables (time, lat, lon)\n",
    "        - static variables (lat, lon)\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_dyn_forcing,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_dyn_forcing=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None\n",
    "    ):\n",
    "\n",
    "        '''\n",
    "        Initialize the ERA5_and_Forcing_Dataset\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing data.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - one_shot(bool, optional): Whether to return all states or just\n",
    "                                    the final state of the training target. Default is None\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical_ERA5_images,\n",
    "                                                 target_ERA5_images,\n",
    "                                                 datetime index, and additional information.\n",
    "        '''\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times \n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # upper-air files\n",
    "\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "        \n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "            \n",
    "        self.all_files = all_files\n",
    "        \n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {} # <------ change\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [len(ERA5_xarray['time']),\n",
    "                                                ind_start,\n",
    "                                                ind_start + len(ERA5_xarray['time'])]\n",
    "            ind_start += len(ERA5_xarray['time']) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "\n",
    "        # ======================================================== #\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "\n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "\n",
    "            for fn in filename_dyn_forcing:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_dyn_forcing)\n",
    "\n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "\n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "\n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "        \n",
    "        if self.filename_diagnostic is not None:\n",
    "\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "            \n",
    "            for fn in filename_diagnostic:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_diagnostic)\n",
    "                \n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "                \n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(filename_forcing), 'Cannot find forcing file [{}]'.format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(filename_static), 'Cannot find static file [{}]'.format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "            \n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        self.start_index = self._get_random_start_index()\n",
    "        self.forecast_step = 0\n",
    "        self.total_length = len(self.ERA5_indices)\n",
    "\n",
    "    def _get_random_start_index(self):\n",
    "        \"\"\"Generate a random start index based on the length of the dataset.\"\"\"\n",
    "        dataset_length = len(self)\n",
    "        return random.randint(0, dataset_length - 1)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray['time']) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        self.forecast_step = 0\n",
    "        self.start_index = self._get_random_start_index()\n",
    "\n",
    "    def _get_new_start_index(self, worker_id=0, num_workers=1):\n",
    "        # Divide the data among workers such that there's no overlap\n",
    "        total_steps = len(self.ERA5_indices) // num_workers\n",
    "        worker_offset = worker_id * total_steps\n",
    "        return worker_offset + (self.start_index % total_steps)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # worker_info = get_worker_info()\n",
    "        # worker_id = worker_info.id if worker_info else 0\n",
    "\n",
    "        # # We are ignoring the handed index in order to manage order in non-sequential dataset\n",
    "        if (self.forecast_step == self.forecast_len + 1): # we are done with this forecast\n",
    "            self.start_index = self._get_random_start_index()\n",
    "            self.forecast_step = 0\n",
    "            index = self.start_index\n",
    "        else: # start a new forecast\n",
    "            self.start_index = self.start_index + 1\n",
    "            index = self.start_index\n",
    "\n",
    "        # select the ind_file based on the iter index \n",
    "        ind_file = find_key_for_number(index, self.ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = self.ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = index - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(self.all_files[int(ind_file)]['time'])-(self.history_len+self.forecast_len+1)\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension\n",
    "        \n",
    "        ind_end_in_file = ind_start_in_file+self.history_len\n",
    "        \n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = self.all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge surface into the dataset\n",
    "\n",
    "        if self.surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = self.surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "            \n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(surface_subset) # <-- lazy merge, ERA5 and surface both not loaded\n",
    "\n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with dynamic forcing, forcing, and static\n",
    "\n",
    "        # the ind_end of the ERA5_subset\n",
    "        ind_end_time = len(ERA5_subset['time'])\n",
    "\n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "\n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if self.dyn_forcing_files:\n",
    "            dyn_forcing_subset = self.dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if self.xarray_forcing:\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(np.array(self.xarray_forcing['time']))\n",
    "            month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time'])) # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = self.xarray_forcing.isel(time=ind_forcing).load() # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "            # ------------------------------------------------------------------------------- #\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if self.xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset['time'])\n",
    "            static_subset_input = self.xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "\n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, self.history_len, self.skip_periods)).load() # <-- load into memory\n",
    "\n",
    "            # update \n",
    "            static_subset_input['time'] = historical_ERA5_images['time']\n",
    "\n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "        \n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "\n",
    "        target_ERA5_images = ERA5_subset.isel(time=slice(-1, None)).load() # <-- load into memory\n",
    "        \n",
    "        ## merge diagnoisc input here:\n",
    "        if self.diagnostic_files:\n",
    "            diagnostic_subset = self.diagnostic_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            \n",
    "            diagnostic_subset = diagnostic_subset.isel(\n",
    "                time=slice(-1, None)).load() # <-- load into memory\n",
    "            \n",
    "            target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "            \n",
    "        # pipe xarray datasets to the sampler\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number\n",
    "        )\n",
    "\n",
    "        # ==================================== #\n",
    "        # data normalization\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # assign sample index\n",
    "        sample[\"datetime\"] = datetime_as_number\n",
    "        sample[\"forecast_step\"] = self.forecast_step + 1\n",
    "        sample[\"index\"] = index\n",
    "        sample[\"stop_forecast\"] = (self.forecast_step == self.forecast_len)\n",
    "\n",
    "        # update the step count\n",
    "        self.forecast_step += 1\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2642f234-554b-4849-bb6e-f332f986fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_MultiStep(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=False,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160d97ee-9518-4cb1-923e-79705a58892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(10):\n",
    "#     sample = dataset.__getitem__(k)\n",
    "#     print(k, sample[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78ca0f6-fb8f-4ec1-b036-5211b175e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b940a071-91ae-4dd3-bc94-6327f8877686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "# num_workers = 1\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,  # Adjust the batch size as needed\n",
    "#     shuffle=False,   # Shuffle the dataset if needed\n",
    "#     num_workers=num_workers,  # Number of subprocesses to use for data loading (adjust as needed)\n",
    "#     drop_last=True,  # Drop the last incomplete batch if not divisible by batch_size,\n",
    "#     prefetch_factor=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c1dc0a-3f8b-4db7-9656-2c81637b0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.dataset.set_epoch(0)\n",
    "# for (k, sample) in enumerate(dataloader):\n",
    "#     print(k, sample[\"forecast_step\"], sample[\"index\"], sample[\"datetime\"], sample[\"stop_forecast\"])\n",
    "#     if k == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d34501-7fba-4086-9a80-27d1628593db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.dataset.set_epoch(3)\n",
    "# for (k, sample) in enumerate(dataloader):\n",
    "#     print(k, sample[\"forecast_step\"], sample[\"index\"], sample[\"datetime\"], sample[\"stop_forecast\"])\n",
    "#     if k == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d8a9f-d5cc-437a-8978-a7c476c6f58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4478d979-f6cc-4f76-9eda-acc30a5d41f4",
   "metadata": {},
   "source": [
    "### Another variation using our worker method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ca315ad-306a-4c6a-9010-ea5ba9854e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "def worker(\n",
    "    tuple_index: Tuple[int, int],\n",
    "    ERA5_indices: Dict[str, List[int]],\n",
    "    all_files: List[Any],\n",
    "    surface_files: Optional[List[Any]],\n",
    "    dyn_forcing_files: Optional[List[Any]],\n",
    "    diagnostic_files: Optional[List[Any]],\n",
    "    xarray_forcing: Optional[Any],\n",
    "    xarray_static: Optional[Any],\n",
    "    history_len: int,\n",
    "    forecast_len: int,\n",
    "    skip_periods: int,\n",
    "    transform: Optional[Callable]\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    '''\n",
    "    Processes a given index to extract and transform data for a specific time slice.\n",
    "\n",
    "    Parameters:\n",
    "    - tuple_index (Tuple[int, int]): Tuple containing the current index and sub-index for processing.\n",
    "    - ERA5_indices (Dict[str, List[int]]): Dictionary containing ERA5 indices metadata.\n",
    "    - all_files (List[Any]): List of xarray datasets containing upper air data.\n",
    "    - surface_files (Optional[List[Any]]): List of xarray datasets containing surface data.\n",
    "    - dyn_forcing_files (Optional[List[Any]]): List of xarray datasets containing dynamic forcing data.\n",
    "    - diagnostic_files (Optional[List[Any]]): List of xarray datasets containing diagnostic data.\n",
    "    - history_len (int): Length of the history sequence.\n",
    "    - forecast_len (int): Length of the forecast sequence.\n",
    "    - skip_periods (int): Number of periods to skip between samples.\n",
    "    - xarray_forcing (Optional[Any]): xarray dataset containing forcing data.\n",
    "    - xarray_static (Optional[Any]): xarray dataset containing static data.\n",
    "\n",
    "    - transform (Optional[Callable]): Transformation function to apply to the data.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: A dictionary containing historical ERA5 images, target ERA5 images, datetime index, and additional information.\n",
    "    '''\n",
    "\n",
    "    index, ind_start_current_step = tuple_index\n",
    "\n",
    "    try:\n",
    "        # select the ind_file based on the iter index\n",
    "        ind_file = find_key_for_number(ind_start_current_step, ERA5_indices)\n",
    "\n",
    "        # get the ind within the current file\n",
    "        ind_start = ERA5_indices[ind_file][1]\n",
    "        ind_start_in_file = ind_start_current_step - ind_start\n",
    "\n",
    "        # handle out-of-bounds\n",
    "        ind_largest = len(all_files[int(ind_file)]['time']) - (history_len + forecast_len + 1)\n",
    "        if ind_start_in_file > ind_largest:\n",
    "            ind_start_in_file = ind_largest\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # subset xarray on time dimension & load it to the memory\n",
    "        \n",
    "        ind_end_in_file = ind_start_in_file+history_len+forecast_len\n",
    "        \n",
    "        ## ERA5_subset: a xarray dataset that contains training input and target (for the current batch)\n",
    "        ERA5_subset = all_files[int(ind_file)].isel(\n",
    "            time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "        \n",
    "        if surface_files:\n",
    "            ## subset surface variables\n",
    "            surface_subset = surface_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1)) #.load() NOT load into memory\n",
    "            \n",
    "            ## merge upper-air and surface here:\n",
    "            ERA5_subset = ERA5_subset.merge(surface_subset) # <-- lazy merge, ERA5 and surface both not loaded\n",
    "        \n",
    "        # ==================================================== #\n",
    "        # split ERA5_subset into training inputs and targets\n",
    "        #   + merge with forcing and static\n",
    "        \n",
    "        # the ind_end of the ERA5_subset\n",
    "        # ind_end_time = len(ERA5_subset['time'])\n",
    "        \n",
    "        # datetiem information as int number (used in some normalization methods)\n",
    "        datetime_as_number = ERA5_subset.time.values.astype('datetime64[s]').astype(int)\n",
    "        \n",
    "        # ==================================================== #\n",
    "        # xarray dataset as input\n",
    "        ## historical_ERA5_images: the final input\n",
    "        \n",
    "        historical_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge dynamic forcing inputs\n",
    "        if dyn_forcing_files:\n",
    "            dyn_forcing_subset = dyn_forcing_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            dyn_forcing_subset = dyn_forcing_subset.isel(\n",
    "                time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "            \n",
    "            historical_ERA5_images = historical_ERA5_images.merge(dyn_forcing_subset)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge forcing inputs\n",
    "        if xarray_forcing:\n",
    "            # =============================================================================== #\n",
    "            # matching month, day, hour between forcing and upper air [time]\n",
    "            # this approach handles leap year forcing file and non-leap-year upper air file\n",
    "            month_day_forcing = extract_month_day_hour(np.array(xarray_forcing['time']))\n",
    "            month_day_inputs = extract_month_day_hour(np.array(historical_ERA5_images['time'])) # <-- upper air\n",
    "            # indices to subset\n",
    "            ind_forcing, _ = find_common_indices(month_day_forcing, month_day_inputs)\n",
    "            forcing_subset_input = xarray_forcing.isel(time=ind_forcing).load() # <-- load into memory\n",
    "            # forcing and upper air have different years but the same mon/day/hour\n",
    "            # safely replace forcing time with upper air time\n",
    "            forcing_subset_input['time'] = historical_ERA5_images['time']\n",
    "            # =============================================================================== #\n",
    "        \n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(forcing_subset_input)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        # merge static inputs\n",
    "        if xarray_static:\n",
    "            # expand static var on time dim\n",
    "            N_time_dims = len(ERA5_subset['time'])\n",
    "            static_subset_input = xarray_static.expand_dims(dim={\"time\": N_time_dims})\n",
    "            # assign coords 'time'\n",
    "            static_subset_input = static_subset_input.assign_coords({'time': ERA5_subset['time']})\n",
    "        \n",
    "            # slice + load to the GPU\n",
    "            static_subset_input = static_subset_input.isel(\n",
    "                time=slice(0, history_len, skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "            # update \n",
    "            static_subset_input['time'] = historical_ERA5_images['time']\n",
    "        \n",
    "            # merge\n",
    "            historical_ERA5_images = historical_ERA5_images.merge(static_subset_input)\n",
    "\n",
    "        # ==================================================== #\n",
    "        # xarray dataset as target\n",
    "        ## target_ERA5_images: the final target\n",
    "        \n",
    "        # get the next forecast step\n",
    "        target_ERA5_images = ERA5_subset.isel(\n",
    "            time=slice(history_len, history_len+skip_periods, skip_periods)).load() # <-- load into memory\n",
    "        \n",
    "        ## merge diagnoisc input here:\n",
    "        if diagnostic_files:\n",
    "            \n",
    "            # subset diagnostic variables\n",
    "            diagnostic_subset = diagnostic_files[int(ind_file)].isel(\n",
    "                time=slice(ind_start_in_file, ind_end_in_file+1))\n",
    "            \n",
    "            # get the next forecast step\n",
    "            diagnostic_subset = diagnostic_subset.isel(\n",
    "                time=slice(history_len, history_len+skip_periods, skip_periods)\n",
    "            ).load() # <-- load into memory\n",
    "            \n",
    "            # merge into the target dataset\n",
    "            target_ERA5_images = target_ERA5_images.merge(diagnostic_subset)\n",
    "\n",
    "        # create a dict object with input/output tensors\n",
    "        sample = Sample(\n",
    "            historical_ERA5_images=historical_ERA5_images,\n",
    "            target_ERA5_images=target_ERA5_images,\n",
    "            datetime_index=datetime_as_number\n",
    "        )\n",
    "\n",
    "        # data normalization\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "\n",
    "        sample[\"index\"] = index\n",
    "        stop_forecast = ((ind_start_current_step - index) == forecast_len)\n",
    "        sample['forecast_hour'] = ind_start_current_step - index + 1\n",
    "        sample['index'] = index\n",
    "        sample['stop_forecast'] = stop_forecast\n",
    "        sample[\"datetime\"] = [\n",
    "            int(historical_ERA5_images.time.values[0].astype('datetime64[s]').astype(int)),\n",
    "            int(target_ERA5_images.time.values[0].astype('datetime64[s]').astype(int))\n",
    "        ]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing index {tuple_index}: {e}\")\n",
    "        raise\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a84a74-615c-4d50-9aa9-fafba447d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from credit.data import drop_var_from_dataset, get_forward_data, Sample, find_key_for_number, extract_month_day_hour, find_common_indices\n",
    "\n",
    "\n",
    "class ERA5_and_Forcing_MultiStep(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class that works on:\n",
    "        - upper-air variables (time, level, lat, lon)\n",
    "        - surface variables (time, lat, lon)\n",
    "        - dynamic forcing variables (time, lat, lon)\n",
    "        - foring variables (time, lat, lon)\n",
    "        - diagnostic variables (time, lat, lon)\n",
    "        - static variables (lat, lon)\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        varname_upper_air,\n",
    "        varname_surface,\n",
    "        varname_dyn_forcing,\n",
    "        varname_forcing,\n",
    "        varname_static,\n",
    "        varname_diagnostic,\n",
    "        filenames,\n",
    "        filename_surface=None,\n",
    "        filename_dyn_forcing=None,\n",
    "        filename_forcing=None,\n",
    "        filename_static=None,\n",
    "        filename_diagnostic=None,\n",
    "        history_len=2,\n",
    "        forecast_len=0,\n",
    "        transform=None,\n",
    "        seed=42,\n",
    "        rank=0,\n",
    "        world_size=1,\n",
    "        skip_periods=None,\n",
    "        one_shot=None,\n",
    "        max_forecast_len=None\n",
    "    ):\n",
    "\n",
    "        '''\n",
    "        Initialize the ERA5_and_Forcing_Dataset\n",
    "\n",
    "        Parameters:\n",
    "        - varname_upper_air (list): List of upper air variable names.\n",
    "        - varname_surface (list): List of surface variable names.\n",
    "        - varname_dyn_forcing (list): List of dynamic forcing variable names.\n",
    "        - varname_forcing (list): List of forcing variable names.\n",
    "        - varname_static (list): List of static variable names.\n",
    "        - varname_diagnostic (list): List of diagnostic variable names.\n",
    "        - filenames (list): List of filenames for upper air data.\n",
    "        - filename_surface (list, optional): List of filenames for surface data.\n",
    "        - filename_dyn_forcing (list, optional): List of filenames for dynamic forcing data.\n",
    "        - filename_forcing (str, optional): Filename for forcing data.\n",
    "        - filename_static (str, optional): Filename for static data.\n",
    "        - filename_diagnostic (list, optional): List of filenames for diagnostic data.\n",
    "        - history_len (int, optional): Length of the history sequence. Default is 2.\n",
    "        - forecast_len (int, optional): Length of the forecast sequence. Default is 0.\n",
    "        - transform (callable, optional): Transformation function to apply to the data.\n",
    "        - seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "        - skip_periods (int, optional): Number of periods to skip between samples.\n",
    "        - one_shot(bool, optional): Whether to return all states or just\n",
    "                                    the final state of the training target. Default is None\n",
    "        - max_forecast_len (int, optional): Maximum length of the forecast sequence.\n",
    "        - shuffle (bool, optional): Whether to shuffle the data. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - sample (dict): A dictionary containing historical_ERA5_images,\n",
    "                                                 target_ERA5_images,\n",
    "                                                 datetime index, and additional information.\n",
    "        '''\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        # skip periods\n",
    "        self.skip_periods = skip_periods\n",
    "        if self.skip_periods is None:\n",
    "            self.skip_periods = 1\n",
    "\n",
    "        # one shot option\n",
    "        self.one_shot = one_shot\n",
    "\n",
    "        # total number of needed forecast lead times\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "        # set random seed\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "        # max possible forecast len\n",
    "        self.max_forecast_len = max_forecast_len\n",
    "\n",
    "        # ======================================================== #\n",
    "        # upper-air files\n",
    "\n",
    "        all_files = []\n",
    "        filenames = sorted(filenames)\n",
    "\n",
    "        for fn in filenames:\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename=fn)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_upper_air)\n",
    "\n",
    "            # collect yearly datasets within a list\n",
    "            all_files.append(xarray_dataset)\n",
    "\n",
    "        self.all_files = all_files\n",
    "\n",
    "        # get sample indices from ERA5 upper-air files:\n",
    "        ind_start = 0\n",
    "        self.ERA5_indices = {}\n",
    "        for ind_file, ERA5_xarray in enumerate(self.all_files):\n",
    "            # [number of samples, ind_start, ind_end]\n",
    "            self.ERA5_indices[str(ind_file)] = [len(ERA5_xarray['time']),\n",
    "                                                ind_start,\n",
    "                                                ind_start + len(ERA5_xarray['time'])]\n",
    "            ind_start += len(ERA5_xarray['time']) + 1\n",
    "\n",
    "        # ======================================================== #\n",
    "        # surface files\n",
    "        if filename_surface is not None:\n",
    "\n",
    "            surface_files = []\n",
    "            filename_surface = sorted(filename_surface)\n",
    "\n",
    "            for fn in filename_surface:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_surface)\n",
    "\n",
    "                surface_files.append(xarray_dataset)\n",
    "\n",
    "            self.surface_files = surface_files\n",
    "\n",
    "        else:\n",
    "            self.surface_files = False\n",
    "\n",
    "        # dynamic forcing files\n",
    "        if filename_dyn_forcing is not None:\n",
    "\n",
    "            dyn_forcing_files = []\n",
    "            filename_dyn_forcing = sorted(filename_dyn_forcing)\n",
    "\n",
    "            for fn in filename_dyn_forcing:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_dyn_forcing)\n",
    "\n",
    "                dyn_forcing_files.append(xarray_dataset)\n",
    "\n",
    "            self.dyn_forcing_files = dyn_forcing_files\n",
    "\n",
    "        else:\n",
    "            self.dyn_forcing_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # diagnostic file\n",
    "        self.filename_diagnostic = filename_diagnostic\n",
    "\n",
    "        if self.filename_diagnostic is not None:\n",
    "\n",
    "            diagnostic_files = []\n",
    "            filename_diagnostic = sorted(filename_diagnostic)\n",
    "\n",
    "            for fn in filename_diagnostic:\n",
    "\n",
    "                # drop variables if they are not in the config\n",
    "                xarray_dataset = get_forward_data(filename=fn)\n",
    "                xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_diagnostic)\n",
    "\n",
    "                diagnostic_files.append(xarray_dataset)\n",
    "\n",
    "            self.diagnostic_files = diagnostic_files\n",
    "\n",
    "        else:\n",
    "            self.diagnostic_files = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # forcing file\n",
    "        self.filename_forcing = filename_forcing\n",
    "\n",
    "        if self.filename_forcing is not None:\n",
    "            assert os.path.isfile(filename_forcing), 'Cannot find forcing file [{}]'.format(filename_forcing)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_forcing)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_forcing)\n",
    "\n",
    "            self.xarray_forcing = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_forcing = False\n",
    "\n",
    "        # ======================================================== #\n",
    "        # static file\n",
    "        self.filename_static = filename_static\n",
    "\n",
    "        if self.filename_static is not None:\n",
    "            assert os.path.isfile(filename_static), 'Cannot find static file [{}]'.format(filename_static)\n",
    "\n",
    "            # drop variables if they are not in the config\n",
    "            xarray_dataset = get_forward_data(filename_static)\n",
    "            xarray_dataset = drop_var_from_dataset(xarray_dataset, varname_static)\n",
    "\n",
    "            self.xarray_static = xarray_dataset\n",
    "        else:\n",
    "            self.xarray_static = False\n",
    "\n",
    "        self.worker = partial(\n",
    "            worker,\n",
    "            ERA5_indices=self.ERA5_indices,\n",
    "            all_files=self.all_files,\n",
    "            surface_files=self.surface_files,\n",
    "            dyn_forcing_files=self.dyn_forcing_files,\n",
    "            diagnostic_files=self.diagnostic_files,\n",
    "            xarray_forcing=self.xarray_forcing,\n",
    "            xarray_static=self.xarray_static,\n",
    "            history_len=self.history_len,\n",
    "            forecast_len=self.forecast_len,\n",
    "            skip_periods=self.skip_periods,\n",
    "            transform=self.transform\n",
    "        )\n",
    "\n",
    "        self.start_index = None\n",
    "        self.forecast_step = 0\n",
    "        self.total_length = len(self.ERA5_indices)\n",
    "        self.epoch = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Total sequence length of each sample.\n",
    "        self.total_seq_len = self.history_len + self.forecast_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # compute the total number of length\n",
    "        total_len = 0\n",
    "        for ERA5_xarray in self.all_files:\n",
    "            total_len += len(ERA5_xarray['time']) - self.total_seq_len + 1\n",
    "        return total_len\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "        self.forecast_step_count = 0\n",
    "        self.current_index = None\n",
    "        self.initial_index = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if (self.forecast_step_count == self.forecast_len + 1) or (self.current_index is None):\n",
    "            # We've completed the last forecast or we're starting for the first time\n",
    "            # Start a new forecast using the sampler index\n",
    "            self.current_index = index  # self._get_random_start_index()\n",
    "            self.forecast_step_count = 0\n",
    "            index = self.current_index\n",
    "            self.initial_index = self.current_index\n",
    "        else:\n",
    "            # Ignore the sampler index and continue the forecast\n",
    "            self.current_index += 1\n",
    "            index = self.current_index\n",
    "\n",
    "        print(self.forecast_step_count, self.forecast_len, self.current_index)\n",
    "        index_pair = (self.initial_index, index)\n",
    "        # Worker process\n",
    "        sample = self.worker(index_pair)\n",
    "\n",
    "        # update the step count\n",
    "        self.forecast_step += 1\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cca4bb6-f219-4d52-8f57-6300275ad43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERA5_and_Forcing_MultiStep(\n",
    "    varname_upper_air=varname_upper_air,\n",
    "    varname_surface=varname_surface,\n",
    "    varname_dyn_forcing=varname_dyn_forcing,\n",
    "    varname_forcing=varname_forcing,\n",
    "    varname_static=varname_static,\n",
    "    varname_diagnostic=varname_diagnostic,\n",
    "    filenames=all_ERA_files,\n",
    "    filename_surface=surface_files,\n",
    "    filename_dyn_forcing=dyn_forcing_files,\n",
    "    filename_forcing=forcing_files,\n",
    "    filename_static=static_files,\n",
    "    filename_diagnostic=diagnostic_files,\n",
    "    history_len=history_len,\n",
    "    forecast_len=forecast_len,\n",
    "    skip_periods=skip_periods,\n",
    "    one_shot=False,\n",
    "    max_forecast_len=max_forecast_len,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "888b1b91-ab92-4ef7-bc5f-22c8d92827e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,  # Adjust the batch size as needed\n",
    "    shuffle=False,   # Shuffle the dataset if needed\n",
    "    num_workers=num_workers,  # Number of subprocesses to use for data loading (adjust as needed)\n",
    "    drop_last=True,  # Drop the last incomplete batch if not divisible by batch_size,\n",
    "    prefetch_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efaa4a48-238f-4921-b3c4-a97fd78b8e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7 0\n",
      "0 7 1\n",
      "0 tensor([1]) tensor([0]) [tensor([283996800]), tensor([284018400])] tensor([False])\n",
      "0 7 2\n",
      "1 tensor([2]) tensor([0]) [tensor([284018400]), tensor([284040000])] tensor([False])\n",
      "0 7 3\n",
      "2 tensor([3]) tensor([0]) [tensor([284040000]), tensor([284061600])] tensor([False])\n",
      "0 7 4\n",
      "3 tensor([4]) tensor([0]) [tensor([284061600]), tensor([284083200])] tensor([False])\n",
      "0 7 5\n",
      "4 tensor([5]) tensor([0]) [tensor([284083200]), tensor([284104800])] tensor([False])\n",
      "0 7 6\n",
      "5 tensor([6]) tensor([0]) [tensor([284104800]), tensor([284126400])] tensor([False])\n",
      "0 7 7\n",
      "6 tensor([7]) tensor([0]) [tensor([284126400]), tensor([284148000])] tensor([False])\n",
      "0 7 8\n",
      "7 tensor([8]) tensor([0]) [tensor([284148000]), tensor([284169600])] tensor([True])\n",
      "0 7 9\n",
      "8 tensor([9]) tensor([0]) [tensor([284169600]), tensor([284191200])] tensor([False])\n",
      "0 7 10\n",
      "9 tensor([10]) tensor([0]) [tensor([284191200]), tensor([284212800])] tensor([False])\n",
      "0 7 11\n",
      "10 tensor([11]) tensor([0]) [tensor([284212800]), tensor([284234400])] tensor([False])\n",
      "0 7 12\n",
      "11 tensor([12]) tensor([0]) [tensor([284234400]), tensor([284256000])] tensor([False])\n",
      "0 7 13\n",
      "12 tensor([13]) tensor([0]) [tensor([284256000]), tensor([284277600])] tensor([False])\n",
      "0 7 14\n",
      "13 tensor([14]) tensor([0]) [tensor([284277600]), tensor([284299200])] tensor([False])\n",
      "0 7 15\n",
      "14 tensor([15]) tensor([0]) [tensor([284299200]), tensor([284320800])] tensor([False])\n",
      "0 7 16\n",
      "15 tensor([16]) tensor([0]) [tensor([284320800]), tensor([284342400])] tensor([False])\n",
      "0 7 17\n",
      "16 tensor([17]) tensor([0]) [tensor([284342400]), tensor([284364000])] tensor([False])\n",
      "0 7 18\n",
      "17 tensor([18]) tensor([0]) [tensor([284364000]), tensor([284385600])] tensor([False])\n",
      "0 7 19\n",
      "18 tensor([19]) tensor([0]) [tensor([284385600]), tensor([284407200])] tensor([False])\n",
      "0 7 20\n",
      "19 tensor([20]) tensor([0]) [tensor([284407200]), tensor([284428800])] tensor([False])\n",
      "0 7 21\n",
      "20 tensor([21]) tensor([0]) [tensor([284428800]), tensor([284450400])] tensor([False])\n",
      "0 7 22\n",
      "21 tensor([22]) tensor([0]) [tensor([284450400]), tensor([284472000])] tensor([False])\n",
      "0 7 23\n",
      "22 tensor([23]) tensor([0]) [tensor([284472000]), tensor([284493600])] tensor([False])\n",
      "0 7 24\n",
      "23 tensor([24]) tensor([0]) [tensor([284493600]), tensor([284515200])] tensor([False])\n",
      "0 7 25\n",
      "24 tensor([25]) tensor([0]) [tensor([284515200]), tensor([284536800])] tensor([False])\n",
      "0 7 26\n",
      "25 tensor([26]) tensor([0]) [tensor([284536800]), tensor([284558400])] tensor([False])\n"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.set_epoch(0)\n",
    "for (k, sample) in enumerate(dataloader):\n",
    "    print(k, sample[\"forecast_hour\"], sample[\"index\"], sample[\"datetime\"], sample[\"stop_forecast\"])\n",
    "    if k == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00cd0b4c-c2fa-48f5-9fae-580e21e1dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4b830a8-c759-4209-a2a7-550531dd5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latitude_weights(conf):\n",
    "    \"\"\"Calculate latitude-based weights for loss function.\n",
    "\n",
    "    This function calculates weights based on latitude values \n",
    "    to be used in loss functions for geospatial data. The weights \n",
    "    are derived from the cosine of the latitude and normalized \n",
    "    by their mean.\n",
    "\n",
    "    Args:\n",
    "        conf (dict): Configuration dictionary containing the \n",
    "            path to the latitude weights file.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 2D tensor of weights with dimensions \n",
    "            corresponding to latitude and longitude.\n",
    "    \"\"\"\n",
    "    _lat = xr.open_dataset(conf[\"loss\"][\"latitude_weights\"])[\"latitude\"].values\n",
    "    dims_lon = xr.open_dataset(conf[\"loss\"][\"latitude_weights\"])[\"longitude\"].shape[0]\n",
    "\n",
    "    weights = np.cos(np.deg2rad(_lat))\n",
    "    weights = weights / np.mean(weights)\n",
    "    repeated_weights = np.repeat(weights[np.newaxis, :], dims_lon, axis=0)\n",
    "    L = repeated_weights.T\n",
    "\n",
    "    return torch.from_numpy(L).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cf2b9cc-9d1a-4e68-8f60-c1132b239343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "\n",
    "def latitude_weights2(conf):\n",
    "    \"\"\"Calculate latitude-based weights for loss function.\n",
    "    This function calculates weights based on latitude values \n",
    "    to be used in loss functions for geospatial data. The weights \n",
    "    are derived from the cosine of the latitude and normalized \n",
    "    by their mean.\n",
    "    \n",
    "    Args:\n",
    "        conf (dict): Configuration dictionary containing the \n",
    "            path to the latitude weights file.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A 2D tensor of weights with dimensions \n",
    "            corresponding to latitude and longitude.\n",
    "    \"\"\"\n",
    "    # Open the dataset and extract latitude and longitude information\n",
    "    ds = xr.open_dataset(conf[\"loss\"][\"latitude_weights\"])\n",
    "    lat = torch.from_numpy(ds[\"latitude\"].values).float()\n",
    "    lon_dim = ds[\"longitude\"].shape[0]\n",
    "    \n",
    "    # Calculate weights using PyTorch operations\n",
    "    weights = torch.cos(torch.deg2rad(lat))\n",
    "    weights = weights / weights.mean()\n",
    "    \n",
    "    # Create a 2D tensor of weights\n",
    "    L = weights.unsqueeze(1).expand(-1, lon_dim)\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe298866-c8dc-43f5-8d4b-3ffeb3cadd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0059, 0.0059, 0.0059,  ..., 0.0059, 0.0059, 0.0059],\n",
       "        [0.0135, 0.0135, 0.0135,  ..., 0.0135, 0.0135, 0.0135],\n",
       "        [0.0212, 0.0212, 0.0212,  ..., 0.0212, 0.0212, 0.0212],\n",
       "        ...,\n",
       "        [0.0212, 0.0212, 0.0212,  ..., 0.0212, 0.0212, 0.0212],\n",
       "        [0.0135, 0.0135, 0.0135,  ..., 0.0135, 0.0135, 0.0135],\n",
       "        [0.0059, 0.0059, 0.0059,  ..., 0.0059, 0.0059, 0.0059]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude_weights(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d20f6d0-c6f9-4b6c-beda-f82978b2627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0059, 0.0059, 0.0059,  ..., 0.0059, 0.0059, 0.0059],\n",
       "        [0.0135, 0.0135, 0.0135,  ..., 0.0135, 0.0135, 0.0135],\n",
       "        [0.0212, 0.0212, 0.0212,  ..., 0.0212, 0.0212, 0.0212],\n",
       "        ...,\n",
       "        [0.0212, 0.0212, 0.0212,  ..., 0.0212, 0.0212, 0.0212],\n",
       "        [0.0135, 0.0135, 0.0135,  ..., 0.0135, 0.0135, 0.0135],\n",
       "        [0.0059, 0.0059, 0.0059,  ..., 0.0059, 0.0059, 0.0059]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude_weights(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
