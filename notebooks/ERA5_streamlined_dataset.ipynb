{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d579b937-c539-4eb3-b99a-682ed6a99b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from credit.datasets.era5_multistep_batcher import Predict_Dataset_Batcher\n",
    "import yaml\n",
    "from credit.parser import credit_main_parser, predict_data_check\n",
    "import multiprocessing as mp\n",
    "from credit.datasets import setup_data_loading\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.transforms import load_transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, DistributedSampler\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import itertools\n",
    "import os\n",
    "import yaml\n",
    "from credit.models import load_model\n",
    "\n",
    "from credit.samplers import DistributedMultiStepBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3e8bc66-0199-4dba-b532-9faa7f115a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['stop_forecast', 'dynamic_forcing', 'prognostic', 'static'])\n",
      "dict_keys(['stop_forecast', 'dynamic_forcing'])\n",
      "dict_keys(['stop_forecast', 'dynamic_forcing', 'prognostic'])\n",
      "dict_keys(['stop_forecast', 'dynamic_forcing', 'prognostic'])\n"
     ]
    }
   ],
   "source": [
    "class ERA5Dataset(Dataset):\n",
    "    \n",
    "    \"\"\" Pytorch Dataset for processed ERA5 data. Relies on a configuration dictionary to define:\n",
    "            1) 2D / 3D variables\n",
    "            2) Start, End and Frequency of Datetimes\n",
    "            3) the base path to the directory where the data is stored. \n",
    "            4) Example YAML Format:\n",
    "            \n",
    "                data:\n",
    "                  source:\n",
    "                    ERA5:\n",
    "                      vars_3D: ['T', 'U', 'V', 'Q']\n",
    "                      vars_2D: ['T500', 'U500', 'V500', 'Q500' ,'Z500', 'tsi', 't2m','SP']\n",
    "                      vars_persist: None\n",
    "                      path: \"/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/\"\n",
    "                \n",
    "                  start_datetime: \"2017-01-01\" \n",
    "                  end_datetime: \"2019-12-31\"\n",
    "                  timestep: \"6h\"\n",
    "        \n",
    "        Assumptions:\n",
    "            1) The data must be stored in yearly zarr files with a unique 4-digit year (YYYY) in the file name\n",
    "            2) \"time\" dimension / coordinate is present with the datetime64[ns] datatype\n",
    "            3) \"level\" dimension name representing the vertical level\n",
    "            4) Dimention order of ('time', level', 'latitude', 'longitude') for 3D vars (remove level for 2D)\n",
    "            5) Stored Zarr data should be chunked efficiently for a fast read (recommend small chunks across time dimension).\n",
    "            \n",
    "            \"\"\" \n",
    "    def __init__(self, config, time_config, source):\n",
    "        \n",
    "        self.source_name = source\n",
    "\n",
    "        # valid sampling modes\n",
    "        self.valid_sampling_modes = [\"init\", \"forcing\", \"y\", \"stop\"]\n",
    "\n",
    "        # time config\n",
    "        self.timestep = time_config['timestep']\n",
    "        self.num_forecast_steps = time_config[\"num_forecast_steps\"]\n",
    "        self.start_datetime = pd.Timestamp(time_config['start_datetime'])\n",
    "        self.end_datetime = pd.Timestamp(time_config['end_datetime'])\n",
    "        \n",
    "        self.init_times = self._timestamps()\n",
    "        self.years = [str(y) for y in self.init_times.year] # only unique years\n",
    "\n",
    "        self.file_dict = {}\n",
    "        self.var_dict = {}\n",
    "        # handle variables and their files\n",
    "        for field_type, d in config['data']['source'][self.source_name].items(): #prognostic, diagnostic, dynamic forcing, static\n",
    "            # print(field_type, d)\n",
    "            if isinstance(d, dict):\n",
    "                files = sorted(glob(d.get(\"path\", \"\")))\n",
    "                # stores a dict to lookup files from that field\n",
    "                self.file_dict[field_type] = self._map_files(files) if files else None\n",
    "                \n",
    "                self.var_dict[field_type] = {\n",
    "                        \"vars_3D\": d.get(\"vars_3D\", []),\n",
    "                        \"vars_2D\": d.get(\"vars_2D\", []),\n",
    "                                           }\n",
    "            else:\n",
    "                self.file_dict[field_type] = None\n",
    "        \n",
    "    def _timestamps(self):\n",
    "        return pd.date_range(self.start_datetime,\n",
    "                                       self.end_datetime - self.num_forecast_steps * self.timestep,\n",
    "                                       freq=self.timestep)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.init_times)\n",
    "        \n",
    "    def _map_files(self, file_list):\n",
    "        \n",
    "        \"\"\" Create a dictionary to lookup the file for a timestep \"\"\"\n",
    "\n",
    "        if len(file_list) > 1:\n",
    "            file_map = {int(y): f for f in file_list for y in self.years if y in f}\n",
    "        else:\n",
    "            file_map = {int(y): file_list[0] for y in self.years}\n",
    "            \n",
    "        \n",
    "        return file_map\n",
    "        \n",
    "    def __getitem__(self, args):\n",
    "        ts, mode = args\n",
    "\n",
    "        return_data = {\"mode\": mode,\n",
    "                      \"stop_forecast\": mode == \"stop\"}\n",
    "\n",
    "        return_data = self._open_ds_extract_fields(\"dynamic_forcing\", ts, return_data)\n",
    "        if mode == \"forcing\":\n",
    "            return return_data\n",
    "\n",
    "        # load prognostic for the remaining modes\n",
    "        return_data = self._open_ds_extract_fields(\"prognostic\", ts, return_data)\n",
    "        if mode == \"init\":\n",
    "            # load static\n",
    "            return_data = self._open_ds_extract_fields(\"static\", ts, return_data)\n",
    "            return return_data\n",
    "\n",
    "        if mode == \"y\" or mode == \"stop\":\n",
    "            # load diagnostic\n",
    "            return_data = self._open_ds_extract_fields(\"diagnostic\", ts, return_data)\n",
    "            return return_data\n",
    "        \n",
    "        raise ValueError(f\"{mode} is not a valid sampling mode in {self.valid_sampling_modes}\")\n",
    "\n",
    "    def _open_ds_extract_fields(self, field_type, ts, return_data):\n",
    "        \"\"\"\n",
    "        opens the dataset, reshapes and concats the variables into an np array, \n",
    "        packs it into the return dict if the data exists\n",
    "        \"\"\"\n",
    "        if self.file_dict[field_type]: #if the file map is not None, do the op\n",
    "            ds = xr.open_dataset(self.file_dict[field_type][ts.year])\n",
    "            if field_type != \"static\":\n",
    "                ds = ds.sel(time=ts)\n",
    "            ds_3D = ds[self.var_dict[field_type][\"vars_3D\"]]\n",
    "            ds_2D = ds[self.var_dict[field_type][\"vars_2D\"]]\n",
    "    \n",
    "            data_np = self._reshape_and_concat(ds_3D, ds_2D)\n",
    "\n",
    "            if data_np.size > 0:\n",
    "                return_data[field_type] = torch.tensor(data_np).float()\n",
    "\n",
    "        return return_data\n",
    "\n",
    "    def _reshape_and_concat(self, ds_3D, ds_2D):\n",
    "\n",
    "        \"\"\" Stack 3D variables along level and variable, concatenate with 2D variables, and reorder dimesions. \"\"\" \n",
    "\n",
    "        # for 3D, order by variables (according to the order in config file) then levels\n",
    "        data_list = []\n",
    "        if ds_3D:\n",
    "            data_3D = ds_3D.to_array().stack({'level_var':['variable', 'level']}).values\n",
    "            data_3D = np.expand_dims(data_3D.transpose(2, 0, 1), axis=1)\n",
    "            data_list.append(data_3D)\n",
    "        if ds_2D:\n",
    "            data_2D = np.expand_dims(ds_2D.to_array().values, axis=1)\n",
    "            data_list.append(data_2D)\n",
    "\n",
    "        combined_data = np.concatenate(data_list, axis=0)\n",
    "        \n",
    "        return combined_data\n",
    "\n",
    "\n",
    "##### WRAPPER TO COMBINE DATASETS ####\n",
    "\n",
    "class DataSourcer(Dataset):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.sources = list(config[\"data\"][\"source\"].keys())\n",
    "        self.start_datetime = config['data']['start_datetime']\n",
    "        self.end_datetime = config['data']['end_datetime']\n",
    "        self.timestep = config['data']['timestep']\n",
    "        self.datetimes = pd.date_range(self.start_datetime, self.end_datetime, freq=self.timestep)\n",
    "        self.forecast_step = 1\n",
    "        self.datasets = [DataBatcher(config, source) for source in self.sources]\n",
    "\n",
    "        self.init_times = self.dataset[0].init_times\n",
    "\n",
    "    def __len__(self):\n",
    "        return min([len(dataset) for dataset in self.datasets])\n",
    "\n",
    "    def __getitem__(self, args):\n",
    "        \n",
    "        ts, mode = args\n",
    "        return torch.concat([ds[(ts, mode)] for ds in self.datasets], dim=0)\n",
    "\n",
    "path = \"/glade/u/home/dkimpara/miles-credit/config/era5_new_data_config.yaml\"\n",
    "with open(path) as cnfg:\n",
    "    config = yaml.safe_load(cnfg)\n",
    "\n",
    "data_config = config[\"data\"]\n",
    "\n",
    "time_config = {\n",
    "    \"timestep\": pd.Timedelta(data_config[\"timestep\"]),\n",
    "    \"num_forecast_steps\": data_config[\"forecast_len\"] + 1,\n",
    "    \"start_datetime\": data_config[\"start_datetime\"],\n",
    "    \"end_datetime\": data_config[\"end_datetime\"]\n",
    "}\n",
    "source = \"ERA5\"\n",
    "dataset = ERA5Dataset(config, time_config, source)\n",
    "\n",
    "ts = dataset.init_times[0]\n",
    "\n",
    "for mode in dataset.valid_sampling_modes:\n",
    "    print(dataset[(ts, mode)].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec621a-1b2f-4088-82ca-948fdabeb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # # load forcing data\n",
    "        # forcing_3D, forcing_2D = self._open_ds_extract_fields(\"dynamic_forcing\", ts)\n",
    "        # forcing_np = self._reshape_and_concat(forcing_3D, forcing_2D)\n",
    "        # if forcing_np:\n",
    "        #     return_data[\"x_forcing\"] = torch.tensor(forcing_np).float()\n",
    "                \n",
    "        # if mode == \"forcing\": # return if all thats needed\n",
    "        #     return return_data\n",
    "\n",
    "        # # load prognostic data\n",
    "        # prog_3D, prog_2D =  self._open_ds_extract_fields(\"prognostic\", ts)\n",
    "        # prog_np = self._reshape_and_concat(prog_3D, prog_2D)\n",
    "        # return_data[\"x\"] = torch.tensor(prog_np).float()\n",
    "\n",
    "        # if mode == \"init\":\n",
    "        #     # load static\n",
    "        #     static_3D, static_2D =  self._open_ds_extract_fields(\"static\", ts)\n",
    "        #     static_np = self._reshape_and_concat(static_3D, static_2D)\n",
    "        #     return_data[\"static\"] = torch.tensor(static_np).float()\n",
    "            \n",
    "        #     return return_data\n",
    "        # if mode == \"y\" or mode == \"stop\":\n",
    "        #     # load diagnostic \n",
    "        #     diag_3D, static_2D =  self._open_ds_extract_fields(\"static\", ts)\n",
    "        #     static_np = self._reshape_and_concat(static_3D, static_2D)\n",
    "        #     return_data[\"static\"] = torch.tensor(static_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772f207-cd8b-4c71-8f93-c60bce3bf225",
   "metadata": {},
   "source": [
    "# timestamp samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02dfc1b7-fa98-4e41-88db-a079312113b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d9beb-9ef3-4ffa-b252-601290fed96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit",
   "language": "python",
   "name": "credit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
