{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d579b937-c539-4eb3-b99a-682ed6a99b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from credit.datasets.era5_multistep_batcher import Predict_Dataset_Batcher\n",
    "import yaml\n",
    "from credit.parser import credit_main_parser, predict_data_check\n",
    "import multiprocessing as mp\n",
    "from credit.datasets import setup_data_loading\n",
    "from credit.forecast import load_forecasts\n",
    "from credit.transforms import load_transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, DistributedSampler\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Extra\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e8bc66-0199-4dba-b532-9faa7f115a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5Dataset(Dataset):\n",
    "    \n",
    "    \"\"\" Pytorch Dataset for processed ERA5 data. Relies on a configuration dictionary to define:\n",
    "            1) 2D / 3D variables\n",
    "            2) Start, End and Frequency of Datetimes\n",
    "            3) the base path to the directory where the data is stored. \n",
    "            4) Example YAML Format:\n",
    "            \n",
    "                data:\n",
    "                  source:\n",
    "                    ERA5:\n",
    "                      vars_3D: ['T', 'U', 'V', 'Q']\n",
    "                      vars_2D: ['T500', 'U500', 'V500', 'Q500' ,'Z500', 'tsi', 't2m','SP']\n",
    "                      vars_persist: None\n",
    "                      path: \"/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/\"\n",
    "                \n",
    "                  start_datetime: \"2017-01-01\" \n",
    "                  end_datetime: \"2019-12-31\"\n",
    "                  time_step: \"6h\"\n",
    "        \n",
    "        Assumptions:\n",
    "            1) The data must be stored in yearly zarr files with a unique 4-digit year (YYYY) in the file name\n",
    "            2) \"time\" dimension / coordinate is present with the datetime64[ns] datatype\n",
    "            3) \"level\" dimension name representing the vertical level\n",
    "            4) Dimention order of ('time', level', 'latitude', 'longitude') for 3D vars (remove level for 2D)\n",
    "            5) Stored Zarr data should be chunked efficiently for a fast read (recommend small chunks across time dimension).\n",
    "            \n",
    "            \"\"\" \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.source_name = \"ERA5\"\n",
    "        self.base_path = config['data']['source'][self.source_name]['path']\n",
    "        self.file_list = sorted(glob(join(self.base_path, \"*\")))\n",
    "        self.start_datetime = config['data']['start_datetime']\n",
    "        self.end_datetime = config['data']['end_datetime']\n",
    "        self.time_step = config['data']['time_step']\n",
    "        self.datetimes = pd.date_range(self.start_datetime, self.end_datetime, freq=self.time_step)\n",
    "        self.vars_2D = config['data']['source'][self.source_name]['vars_2D']\n",
    "        self.vars_3D = config['data']['source'][self.source_name]['vars_3D']\n",
    "        self.forecast_step = 5\n",
    "        self.return_target = False\n",
    "        self.files = self._map_files()\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.datetimes) - self.forecast_step\n",
    "        \n",
    "    def __getitem__(self, args):\n",
    "        idx = int(args[0])\n",
    "        dataset_x = self._open_file(self.files[idx], self.datetimes[idx])\n",
    "        data_array_x = self._reshape_and_concat(dataset_x)\n",
    "        \n",
    "        if self.return_target:\n",
    "            \n",
    "            idx_y = idx + 1\n",
    "            dataset_y = self._open_file(self.files[idx_y], self.datetimes[idx_y])\n",
    "            data_array_y = self._reshape_and_concat(dataset_y)\n",
    "            \n",
    "            return torch.from_numpy(data_array_x).float(), torch.from_numpy(data_array_y).float()\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            return torch.from_numpy(data_array_x).float(), self.datetimes[idx].strftime(\"%Y%m%d_%H00\"), idx\n",
    "        \n",
    "    def _map_files(self):\n",
    "        \n",
    "        \"\"\" Create a list of files that contain the data for a given time step. \"\"\"\n",
    "\n",
    "        years = [str(y) for y in self.datetimes.year]\n",
    "        self.file_map = {int(y): f for f in self.file_list for y in years if y in f}\n",
    "\n",
    "        return [self.file_map[d.year] for d in self.datetimes]\n",
    "\n",
    "    def _open_file(self, filename, datetime):\n",
    "\n",
    "        \"\"\" Open a specific file and subset a specific time step. \"\"\"\n",
    "\n",
    "        data = xr.open_zarr(filename).sel(time=datetime)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def _reshape_and_concat(self, data):\n",
    "\n",
    "        \"\"\" Stack 3D variables along level and variable, concatenate with 2D variables, and reorder dimesions. \"\"\" \n",
    "        \n",
    "        data_3D = data[self.vars_3D].to_array().stack({'level_var':['variable', 'level']}).values\n",
    "        data_3D = np.expand_dims(data_3D.transpose(2, 0, 1), axis=1)\n",
    "\n",
    "        data_2D = np.expand_dims(data[self.vars_2D].to_array().values, axis=1)\n",
    "        \n",
    "        combined_data = np.concatenate([data_3D, data_2D])\n",
    "\n",
    "        return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f807eb-6835-4954-a459-cf57bd24e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepBatchSamplerSubset(Sampler):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        index_subset = None,  # if None, use entire dataset\n",
    "        batch_size=3,\n",
    "        num_forecast_steps=4,\n",
    "        backprop_forecast_steps=[], # list from 1 to forecast_steps\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        taking advantage of DistributedSampler class code with this dataset.\n",
    "        can be used on its own with index_subset=None\n",
    "        \n",
    "        Args:\n",
    "            data: list of data\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "        if index_subset is not None and len(index_subset) > 0:\n",
    "            # don't need to shuffle because distributed sampler shuffles for us.\n",
    "            self.index_subset = torch.tensor(\n",
    "                index_subset\n",
    "            )  # must all be valid starting times, this is given by the DistributedSampler Wrapper\n",
    "        else:\n",
    "            self.index_subset = torch.randperm(len(dataset))\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_forecast_steps = num_forecast_steps\n",
    "        if backprop_forecast_steps:\n",
    "            self.backprop_forecast_steps = backprop_forecast_steps\n",
    "        else:\n",
    "            self.backprop_forecast_steps = list(range(1, self.num_forecast_steps+1))\n",
    "\n",
    "            \n",
    "        self.num_start_batches = (\n",
    "            len(self.index_subset) + self.batch_size - 1\n",
    "        ) // self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # actual number of iters of the sampler\n",
    "        return self.num_start_batches * self.num_forecast_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        index_iter = iter(self.index_subset)\n",
    "\n",
    "        batch = list(itertools.islice(index_iter, self.batch_size))\n",
    "        \n",
    "        while batch:\n",
    "            # iterate through batches of valid starting times,\n",
    "            # wrt self.num_forecast_steps\n",
    "            for i in range(self.num_forecast_steps + 1):\n",
    "                # for each batch of valid starting times,\n",
    "                # iterate through subsequent valid forecast times\n",
    "                \n",
    "                if i == 0:\n",
    "                    yield [(k + i, \"init\") for k in batch]\n",
    "                    \n",
    "                elif i in self.backprop_forecast_steps:\n",
    "                    yield [(k + i, \"backprop\") for k in batch]\n",
    "                    \n",
    "                else:\n",
    "                    yield [(k + i, \"forcing\") for k in batch]\n",
    "\n",
    "            batch = list(itertools.islice(index_iter, self.batch_size))\n",
    "\n",
    "\n",
    "class DistributedMultiStepBatchSampler(DistributedSampler):\n",
    "    \n",
    "    def __init__(self, dataset: Dataset,\n",
    "                 batch_size: int,\n",
    "                 num_forecast_steps: int,\n",
    "                 backprop_forecast_steps=[],\n",
    "                 num_replicas = None,\n",
    "                 rank = None, shuffle: bool = True,\n",
    "                 seed: int = 0, drop_last: bool = False,\n",
    "                 ) -> None:\n",
    "        \n",
    "        super().__init__(dataset=dataset, num_replicas=num_replicas,\n",
    "                         rank=rank, shuffle=shuffle, seed=seed,\n",
    "                         drop_last=drop_last)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_forecast_steps = num_forecast_steps\n",
    "        self.backprop_forecast_steps = backprop_forecast_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        indices = list(super().__iter__())\n",
    "\n",
    "        batch_sampler = MultiStepBatchSamplerSubset(self.dataset,\n",
    "                                                    indices,\n",
    "                                                    batch_size=self.batch_size,\n",
    "                                                    num_forecast_steps=self.num_forecast_steps,\n",
    "                                                    backprop_forecast_steps=self.backprop_forecast_steps,\n",
    "                                                    )\n",
    "        return iter(batch_sampler)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \n",
    "        return self.num_samples * self.num_forecast_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f0225d-fcf1-4822-97c5-f22d070911aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/glade/work/cbecker/notebooks/credit_test_yaml.yaml\"\n",
    "with open(path) as cnfg:\n",
    "    config = yaml.safe_load(cnfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c93b0e-76f4-4baa-8cba-c0d303b44105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'source': {'ERA5': {'vars_3D': ['T', 'U', 'V', 'Q'],\n",
       "    'vars_2D': ['T500', 'U500', 'V500', 'Q500', 'Z500', 'tsi', 't2m', 'SP'],\n",
       "    'vars_persist': 'None',\n",
       "    'path': '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_mlevel_cesm_stage1/all_in_one/'}},\n",
       "  'start_datetime': '2017-01-01',\n",
       "  'end_datetime': '2019-12-31',\n",
       "  'time_step': '6h'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25b32cb-7632-4935-8b2b-8b695aa81dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 0\n",
    "world_size = 2\n",
    "data = list(range(25))\n",
    "dataset = ERA5Dataset(config=config)\n",
    "sampler = DistributedMultiStepBatchSampler(dataset, num_forecast_steps=3, batch_size=6, num_replicas=2, rank=0, shuffle=True)\n",
    "loader = DataLoader(dataset, batch_sampler=sampler, num_workers=4, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dda7d42-7fd1-4b3d-b12e-d584c97a1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for batch_idx, data in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    l.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7b9f76-aec8-42c3-ba73-4373f8cae304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20181214_0000', '20181014_0600', '20170810_1800', '20170731_0600', '20170603_0600', '20180228_1800')\n",
      "('20181214_0600', '20181014_1200', '20170811_0000', '20170731_1200', '20170603_1200', '20180301_0000')\n",
      "('20181214_1200', '20181014_1800', '20170811_0600', '20170731_1800', '20170603_1800', '20180301_0600')\n",
      "('20181214_1800', '20181015_0000', '20170811_1200', '20170801_0000', '20170604_0000', '20180301_1200')\n",
      "('20171020_0000', '20190121_0000', '20190218_0000', '20190226_0000', '20180729_1200', '20190421_1800')\n",
      "('20171020_0600', '20190121_0600', '20190218_0600', '20190226_0600', '20180729_1800', '20190422_0000')\n",
      "('20171020_1200', '20190121_1200', '20190218_1200', '20190226_1200', '20180730_0000', '20190422_0600')\n",
      "('20171020_1800', '20190121_1800', '20190218_1800', '20190226_1800', '20180730_0600', '20190422_1200')\n",
      "('20170501_0000', '20190728_0600', '20170928_0600', '20180715_0600', '20181031_1800', '20171204_0600')\n",
      "('20170501_0600', '20190728_1200', '20170928_1200', '20180715_1200', '20181101_0000', '20171204_1200')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(l[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dffcfc-dd02-4233-84a1-8b8473de629b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CREDIT]",
   "language": "python",
   "name": "conda-env-CREDIT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
